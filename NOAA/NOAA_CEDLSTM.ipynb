{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Flatten, Reshape, Dropout, LSTM,\n",
    "    RepeatVector\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import Voronoi\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from pykrige.ok3d import OrdinaryKriging3D\n",
    "from pykrige.uk3d import UniversalKriging3D\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/content/drive/MyDrive/Physics/Dataset10'\n",
    "save_dir = '/content/drive/MyDrive/Physics/NOAA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(f'{out_dir}/x_NOAA_train.npy',mmap_mode = 'r')\n",
    "y_train = np.load(f'{out_dir}/y_NOAA_train.npy',mmap_mode = 'r')\n",
    "x_test = np.load(f'{out_dir}/x_NOAA_test.npy',mmap_mode = 'r')\n",
    "y_test = np.load(f'{out_dir}/y_NOAA_test.npy',mmap_mode = 'r')\n",
    "\n",
    "\n",
    "x_data = np.load(f'{out_dir}/x_NOAA.npy',mmap_mode = 'r')\n",
    "y_data = np.load(f'{out_dir}/y_NOAA.npy',mmap_mode = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CED Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "# Encoder\n",
    "input_img = layers.Input(shape=(180, 360, 1))\n",
    "x = layers.Conv2D(32, (7, 7), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(16, (7, 7), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)  # Added to match dimensions correctly\n",
    "x = layers.Flatten()(x)\n",
    "encoded = layers.Dense(512, activation='relu', name='encoded')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Dense(45*90*16, activation='relu')(encoded)  # Adjusted to match the encoder output\n",
    "x = layers.Reshape((45, 90, 16))(x)\n",
    "x = layers.Conv2D(16, (7, 7), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(32, (7, 7), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2D(1, (7, 7), activation='linear', padding='same', name='decoded')(x)\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mae')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "tf.keras.utils.set_random_seed(12)\n",
    "\n",
    "\n",
    "# Callback to save the best model\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f'{save_dir}/best_model_CED.h5',  
    "    monitor='val_loss',  
    "    save_best_only=True,  
    "    verbose=1,  
    "    mode='min',  
    "    save_format='h5'  
    ")\n",
    "\n",
    "\n",
    "# Callbacks for adaptive learning rate, early stopping, and model checkpointing\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=40,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=60,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Training the autoencoder\n",
    "history = autoencoder.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    batch_size=16,\n",
    "    callbacks=[reduce_lr, early_stop, model_checkpoint],  # Include model_checkpoint in the callbacks\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained CED Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models.load_model(f'{save_dir}/best_model_CED.h5')\n",
    "\n",
    "encode_layers = autoencoder.layers[:7]\n",
    "\n",
    "# Create a new Sequential model with the extracted layers\n",
    "encoded = tf.keras.models.Sequential(encode_layers)\n",
    "\n",
    "\n",
    "last_layers = autoencoder.layers[-7:]\n",
    "new_model = tf.keras.models.Sequential(last_layers)\n",
    "reconstructed_images = autoencoder.predict(x_test[:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, decoder_model, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        # Ensure the decoder_model is a callable (e.g., a Keras model or a function)\n",
    "        if not callable(decoder_model):\n",
    "            raise ValueError(\"decoder_model must be callable\")\n",
    "        self.decoder_model = decoder_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # It's good practice to check if the inputs are valid for the decoder_model\n",
    "        # This can be more specific based on the expected input shape, type, etc.\n",
    "        if inputs is None:\n",
    "            raise ValueError(\"Input to DecoderLayer cannot be None\")\n",
    "\n",
    "        # Use decoder_model to decode the inputs\n",
    "        decoded_images = self.decoder_model(inputs)\n",
    "        return decoded_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Evaluate CED Reconstructed Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust global font size and colorbar label size\n",
    "mpl.rcParams['font.size'] = 8\n",
    "mpl.rcParams['axes.labelsize'] = 6  # Increased label size for better readability\n",
    "\n",
    "def plot_image_and_colorbar(ax, image, title, mask=None, cmap='viridis', cbar_labelsize=6, vmin=None, vmax=None):\n",
    "    # Apply mask if provided, setting masked values to NaN\n",
    "    if mask is not None:\n",
    "        image = np.where(mask, np.nan, image)\n",
    "    im = ax.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)  # Use vmin and vmax for scaling\n",
    "    ax.set_title(title, fontsize=11)  # Increase title font size\n",
    "    ax.axis('off')  # Hide axes\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=10)  # Reduce tick label size\n",
    "\n",
    "def plot_and_save_comparison_multi(true_images, vor_images, reconstructed_images, indices, dpi=300):\n",
    "    ncols = len(indices)\n",
    "    nrows = 4\n",
    "\n",
    "    # Determine color scale range\n",
    "    vmin = np.nanmin(true_images)\n",
    "    vmax = np.nanmax(true_images)\n",
    "    error_vmax = np.nanmax(np.abs(true_images - reconstructed_images)) - 25\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(3 * ncols, 6), dpi=dpi)\n",
    "\n",
    "    for col, index in enumerate(indices):\n",
    "        mask = true_images[index, :, :, 0] == 0\n",
    "\n",
    "        # Plot actual image\n",
    "        plot_image_and_colorbar(axs[0, col], true_images[index, :, :, 0], \"NOAA State Field\", mask=mask, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        # Plot reconstructed image\n",
    "        plot_image_and_colorbar(axs[1, col], reconstructed_images[index, :, :, 0], \"NOAA Reconstructed Fields\", mask=mask, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        # Plot error map\n",
    "        error_map = np.abs(true_images[index, :, :, 0] - reconstructed_images[index, :, :, 0])\n",
    "        plot_image_and_colorbar(axs[2, col], error_map, \"Error Map\", mask=mask, cmap='viridis', vmin=0, vmax=error_vmax)\n",
    "\n",
    "        # Plot Voronoi image\n",
    "        plot_image_and_colorbar(axs[3, col], vor_images[index, :, :, 0], \"NOAA Voronoi Fields\", mask=mask, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('CED-NOAA.png', dpi=dpi, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_and_save_comparison_multi(y_test, x_test, reconstructed_images, indices=[80, 120, 200, 400], dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask_and_calculate_ssim_psnr(y_true, y_pred, mask):\n",
    "    # Apply mask to y_pred, setting values where mask is True to 0\n",
    "    y_pred_masked = np.where(mask, 0, y_pred)\n",
    "\n",
    "    # Directly calculate SSIM and PSNR\n",
    "    ssim_val = ssim(y_true, y_pred_masked, data_range=y_true.max() - y_true.min())\n",
    "    psnr_val = psnr(y_true, y_pred_masked, data_range=y_true.max() - y_true.min())\n",
    "\n",
    "    return ssim_val, psnr_val\n",
    "\n",
    "# Assume y_test and reconstructed_images are already in np.array format\n",
    "ssim_values = []\n",
    "psnr_values = []\n",
    "\n",
    "# Assume the shape of the mask is the same as each image in y_test and reconstructed_images\n",
    "mask = y_test == 0  # Assume the mask is created based on 0 values in y_test\n",
    "\n",
    "for i in range(y_test.shape[0]):\n",
    "    ssim_val, psnr_val = apply_mask_and_calculate_ssim_psnr(y_test[i, :, :, 0], reconstructed_images[i, :, :, 0], mask[i, :, :, 0])\n",
    "    ssim_values.append(ssim_val)\n",
    "    psnr_values.append(psnr_val)\n",
    "\n",
    "# Calculate average SSIM and PSNR\n",
    "mean_ssim = np.mean(ssim_values)\n",
    "mean_psnr = np.mean(psnr_values)\n",
    "\n",
    "print(f\"Average SSIM: {mean_ssim}\")\n",
    "print(f\"Average PSNR: {mean_psnr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split encoded features into 18 datasets\n",
    "encoded_features = encoded.predict(x_data[:,:,:,0])\n",
    "datasets = np.split(encoded_features, 18)\n",
    "\n",
    "# Randomly select datasets for training and testing\n",
    "train_indices, test_indices = train_test_split(np.arange(18), test_size=1/3, shuffle=False)\n",
    "\n",
    "# Prepare training and testing data\n",
    "def prepare_data(indices, datasets, input_length=3, forecast_horizon=3):\n",
    "    X, y = [], []\n",
    "    for idx in indices:\n",
    "        data = datasets[idx]\n",
    "        for i in range(len(data) - input_length - forecast_horizon + 1):\n",
    "            X.append(data[i:(i + input_length)])\n",
    "            y.append(data[(i + input_length):(i + input_length + forecast_horizon)])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "x_train, y_train = prepare_data(train_indices, datasets)\n",
    "np.save(f'{save_dir}/x_NOAA_LSTM_train.npy', x_train)\n",
    "np.save(f'{save_dir}/y_NOAA_LSTM_train.npy', y_train)\n",
    "\n",
    "x_val, y_val = prepare_data(test_indices, datasets)\n",
    "np.save(f'{save_dir}/x_NOAA_LSTM_val.npy', x_val)\n",
    "np.save(f'{save_dir}/y_NOAA_LSTM_val.npy', y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_NOAA_LSTM_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_LSTM_train.npy', mmap_mode='r')\n",
    "y_NOAA_LSTM_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_LSTM_train.npy', mmap_mode='r')\n",
    "x_NOAA_LSTM_val = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_LSTM_val.npy', mmap_mode='r')\n",
    "y_NOAA_LSTM_val = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_LSTM_val.npy', mmap_mode='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for the training process\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=40,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f'{save_dir}/best_model_ori_NOAA_3steps_mae.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2, \n",
    "    patience=30, \n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.0001, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an optimized LSTM model with custom parameters\n",
    "def build_optimized_model(n_step=3, feature_dim=512):\n",
    "    \"\"\"\n",
    "    Builds an LSTM model optimized with regularization and adjusted complexity.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(n_step, feature_dim))\n",
    "    x = layers.LSTM(128, return_sequences=True)(inputs)  # Reduce LSTM units to balance complexity\n",
    "    x = layers.LSTM(128)(x)\n",
    "    x = layers.RepeatVector(n_step)(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(feature_dim))(x)\n",
    "    outputs = layers.Activation('relu')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"OptimizedModel\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = build_optimized_model()\n",
    "optimized_model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "optimized_model.summary()\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize best validation loss\n",
    "best_seed = None  # Initialize best seed for reproducibility\n",
    "\n",
    "# Loop through different random seeds\n",
    "for seed in range(5):  \n",
    "    tf.keras.backend.clear_session()  \n",
    "    tf.random.set_seed(seed) \n",
    "\n",
    "    # Rebuild and compile the model with the current seed\n",
    "    optimized_model = build_optimized_model()\n",
    "    optimized_model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "\n",
    "    # Train the model\n",
    "    history = optimized_model.fit(\n",
    "        x_NOAA_LSTM_train, y_NOAA_LSTM_train,\n",
    "        validation_data=(x_NOAA_LSTM_val, y_NOAA_LSTM_val),\n",
    "        epochs=150,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    "    )\n",
    "\n",
    "    # Check if the current seed resulted in a better validation loss\n",
    "    if min(history.history['val_loss']) < best_val_loss:\n",
    "        best_val_loss = min(history.history['val_loss'])\n",
    "        best_seed = seed\n",
    "\n",
    "print(f\"Best seed: {best_seed}, Best validation loss: {best_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = models.load_model(f'{save_dir}/best_model_ori_NOAA_3steps_mae.h5')\n",
    "\n",
    "y_test_splitted = np.split(y_test, 6)\n",
    "\n",
    "y_test_cropped = [images[3:-2,:,:, :] for images in y_test_splitted]\n",
    "\n",
    "y_test_final = np.array(y_test_cropped)\n",
    "\n",
    "\n",
    "res_test_lst = lstm.predict(x_NOAA_LSTM_val)\n",
    "res_test_lst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_lst = lstm.predict(x_NOAA_LSTM_val)\n",
    "decoder_layer = DecoderLayer(new_model)\n",
    "\n",
    "restruct_images_lstm = np.array(decoder_layer(res_test_lst[:200,0,:]))\n",
    "restruct_images_true = np.array(decoder_layer(y_NOAA_LSTM_val[:200,0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Evaluate LSTM Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 8\n",
    "mpl.rcParams['axes.labelsize'] = 6  \n",
    "\n",
    "def plot_image_and_colorbar(ax, image, title, mask=None, cmap='viridis', cbar_labelsize=6, vmin=None, vmax=None):\n",
    "    if mask is not None:\n",
    "        image = np.where(mask, np.nan, image)\n",
    "    im = ax.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)  \n",
    "    ax.set_title(title, fontsize=11)  \n",
    "    ax.axis('off')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "    cbar.ax.tick_params(labelsize=10) \n",
    "\n",
    "def plot_and_save_comparison_multi(true_images, vor_images, reconstructed_images, indices, dpi=300):\n",
    "    ncols = len(indices)\n",
    "    nrows = 4\n",
    "\n",
    "    vmin = np.nanmin(true_images)\n",
    "    vmax = np.nanmax(true_images)\n",
    "    error_vmax =4\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(3 * ncols, 6), dpi=dpi)\n",
    "\n",
    "    for col, index in enumerate(indices):\n",
    "        mask = true_images[index, :, :, 0] == 0\n",
    "\n",
    "        plot_image_and_colorbar(axs[0, col], true_images[index, :, :, 0], \"NOAA State Field\", mask=mask, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        plot_image_and_colorbar(axs[1, col], reconstructed_images[index, :, :, 0], \"NOAA LSTM Predicted Fields\", mask=mask, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        error_map = np.abs(true_images[index, :, :, 0] - reconstructed_images[index, :, :, 0])\n",
    "        plot_image_and_colorbar(axs[2, col], error_map, \"Error Map\", mask=mask, cmap='viridis', vmin=0, vmax=error_vmax)\n",
    "\n",
    "        plot_image_and_colorbar(axs[3, col], vor_images[index, :, :, 0], \"NOAA CED Reconstructed Fields\", mask=mask, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('CED-LSTM-NOAA.png', dpi=dpi, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_and_save_comparison_multi( y_test_final[0],restruct_images_true, restruct_images_lstm, indices=[10, 50, 90, 130], dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(original_images, predicted_images):\n",
    "    masked_predicted_images = []\n",
    "    for original, predicted in zip(original_images, predicted_images):\n",
    "        mask = original == 0\n",
    "\n",
    "        masked_predicted = np.where(mask, 0, predicted)\n",
    "        masked_predicted_images.append(masked_predicted)\n",
    "\n",
    "    return masked_predicted_images\n",
    "\n",
    "def calculate_metrics(original_images, predicted_images):\n",
    "    ssim_scores = []\n",
    "    psnr_scores = []\n",
    "    l2_norms = []\n",
    "\n",
    "    for original, predicted in zip(original_images, predicted_images):\n",
    "        # Calculate SSIM\n",
    "        ssim_score = ssim(original, predicted, multichannel=True)\n",
    "        ssim_scores.append(ssim_score)\n",
    "\n",
    "        # Calculate PSNR\n",
    "        psnr_score = psnr(original, predicted, data_range=predicted.max() - predicted.min())\n",
    "        psnr_scores.append(psnr_score)\n",
    "\n",
    "        # Calculate L2 norm\n",
    "        l2_norm = np.linalg.norm(original - predicted)\n",
    "        l2_norms.append(l2_norm)\n",
    "\n",
    "    return ssim_scores, psnr_scores, l2_norms\n",
    "\n",
    "masked_predicted_images = apply_mask(y_test_final[0,:,:,:,:], restruct_images_lstm)\n",
    "\n",
    "ssim_scores, psnr_scores, l2_norms = calculate_metrics(y_test_final[0,:,:,:,:], masked_predicted_images)\n",
    "\n",
    "print(f\"Average SSIM: {np.mean(ssim_scores)}\")\n",
    "print(f\"Average PSNR: {np.mean(psnr_scores)}\")\n",
    "print(f\"Average L2 Norm: {np.mean(l2_norms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "res_test_lst = lstm.predict(x_NOAA_LSTM_val)[:,0,:]\n",
    "\n",
    "end_time = time.time()\n",
    "# Calculate inference time\n",
    "inference_time = end_time - start_time\n",
    "inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Subset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_subset(original_images, predicted_images):\n",
    "    ssim_scores = []\n",
    "    psnr_scores = []\n",
    "    l2_norms = []\n",
    "\n",
    "    for original, predicted in zip(original_images, predicted_images):\n",
    "        mask = original == 0\n",
    "\n",
    "        masked_predicted = np.where(mask, 0, predicted)\n",
    "\n",
    "        # Calculate SSIM\n",
    "        ssim_score = ssim(original, masked_predicted, multichannel=True)\n",
    "        ssim_scores.append(ssim_score)\n",
    "\n",
    "        # Calculate PSNR\n",
    "        psnr_score = psnr(original, masked_predicted, data_range=masked_predicted.max() - masked_predicted.min())\n",
    "        psnr_scores.append(psnr_score)\n",
    "\n",
    "        # Calculate L2 norm\n",
    "        l2_norm = np.linalg.norm(original - masked_predicted)\n",
    "        l2_norms.append(l2_norm)\n",
    "\n",
    "    return ssim_scores, psnr_scores, l2_norms\n",
    "import time\n",
    "\n",
    "# Initialize lists to store metrics for each subset\n",
    "all_ssim_scores = []\n",
    "all_psnr_scores = []\n",
    "all_l2_norms = []\n",
    "all_inference_times = []\n",
    "\n",
    "# Iterate over subsets of images and calculate metrics\n",
    "for i in range(len(y_test_final)):\n",
    "    # Calculate metrics for the current subset\n",
    "    start_time = time.time()\n",
    "    restruct_images_lstm = np.array(decoder_layer(res_test_lst[1035*i:1035*(i+1),:]))\n",
    "    res_test_lst = lstm.predict(x_NOAA_LSTM_val)[:,0,:]\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate inference time\n",
    "    inference_time = end_time - start_time\n",
    "    all_inference_times.append(inference_time)\n",
    "\n",
    "    # Calculate SSIM, PSNR, and L2 norms\n",
    "    ssim_scores, psnr_scores, l2_norms = calculate_metrics_subset(y_test_final[i], restruct_images_lstm)\n",
    "\n",
    "    # Append metrics to the lists\n",
    "    all_ssim_scores.append(ssim_scores)\n",
    "    all_psnr_scores.append(psnr_scores)\n",
    "    all_l2_norms.append(l2_norms)\n",
    "\n",
    "# Compute the average metrics for each subset\n",
    "avg_ssim_scores = [sum(scores) / len(scores) for scores in all_ssim_scores]\n",
    "avg_psnr_scores = [sum(scores) / len(scores) for scores in all_psnr_scores]\n",
    "avg_l2_norms = [sum(norms) / len(norms) for norms in all_l2_norms]\n",
    "avg_inference_time = sum(all_inference_times) / len(all_inference_times)\n",
    "\n",
    "# Print the average metrics for each subset\n",
    "for i in range(len(avg_ssim_scores)):\n",
    "    print(f\"Subset {i+1}:\")\n",
    "    print(f\"Average SSIM: {avg_ssim_scores[i]}\")\n",
    "    print(f\"Average PSNR: {avg_psnr_scores[i]}\")\n",
    "    print(f\"Average L2 Norm: {avg_l2_norms[i]}\")\n",
    "    print(f\"Inference Time: {avg_inference_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
