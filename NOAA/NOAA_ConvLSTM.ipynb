{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Flatten, Reshape, Dropout, LSTM,\n",
    "    RepeatVector\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/content/drive/MyDrive/Physics/Dataset10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(f'{out_dir}/x_NOAA_train.npy',mmap_mode = 'r')\n",
    "y_train = np.load(f'{out_dir}/y_NOAA_train.npy',mmap_mode = 'r')\n",
    "x_test = np.load(f'{out_dir}/x_NOAA_test.npy',mmap_mode = 'r')\n",
    "y_test = np.load(f'{out_dir}/y_NOAA_test.npy',mmap_mode = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    # Initialize the scaled data array with the same shape as the input data\n",
    "    scaled_data = np.zeros_like(data, dtype=np.float32)\n",
    "    # Initialize arrays to store the min and max values for each sample\n",
    "    min_vals = np.zeros((data.shape[0], 1, 1, 1))\n",
    "    max_vals = np.zeros_like(min_vals)\n",
    "\n",
    "    # Iterate through each sample in the data\n",
    "    for i in range(data.shape[0]):\n",
    "        # Calculate the min and max values for the current sample\n",
    "        min_vals[i] = np.min(data[i])\n",
    "        max_vals[i] = np.max(data[i])\n",
    "        # Apply the scaling operation\n",
    "        scaled_data[i] = (data[i] - min_vals[i]) / (max_vals[i] - min_vals[i])\n",
    "\n",
    "    return scaled_data, min_vals, max_vals\n",
    "\n",
    "x_train_scaled,_,_ = scale_data(x_train)\n",
    "x_test_scaled,_,_ = scale_data(x_test)\n",
    "y_train_scaled,_,_ = scale_data(y_train)\n",
    "y_test_scaled,min_vals,max_vals = scale_data(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{out_dir}/x_train_scaled.npy', x_train_scaled)\n",
    "np.save(f'{out_dir}/x_test_scaled.npy', x_test_scaled)\n",
    "np.save(f'{out_dir}/y_train_scaled.npy', y_train_scaled)\n",
    "np.save(f'{out_dir}/y_test_scaled.npy', y_test_scaled)\n",
    "np.save(f'{out_dir}/y_test_min_vals.npy', min_vals)\n",
    "np.save(f'{out_dir}/y_test_max_vals.npy', max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = np.load(f'{out_dir}/x_train_scaled.npy', mmap_mode='r+')\n",
    "x_test_scaled = np.load(f'{out_dir}/x_test_scaled.npy', mmap_mode='r+')\n",
    "y_train_scaled = np.load(f'{out_dir}/y_train_scaled.npy', mmap_mode='r+')\n",
    "y_test_scaled = np.load(f'{out_dir}/y_test_scaled.npy', mmap_mode='r+')\n",
    "x_train_scaled = x_train_scaled.reshape(x_train_scaled.shape[0], 180, 360, 1)\n",
    "x_test_scaled = x_test_scaled.reshape(x_test_scaled.shape[0], 180, 360, 1)\n",
    "\n",
    "min_vals = np.load(f'{out_dir}/y_test_min_vals.npy')\n",
    "max_vals = np.load(f'{out_dir}/y_test_max_vals.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_convLSTM_data(x_data, y_data, num_of_sources, input_length=3, forecast_horizon=3):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Calculate the number of samples per source\n",
    "    samples_per_source = len(x_data) // num_of_sources\n",
    "\n",
    "    for i in range(num_of_sources):\n",
    "        # Calculate start and end indices based on the number of samples per source\n",
    "        start_idx = i * samples_per_source\n",
    "        end_idx = start_idx + samples_per_source\n",
    "\n",
    "        # Slice x_data and y_data to get data for the current source\n",
    "        x_source_data = x_data[start_idx:end_idx]\n",
    "        y_source_data = y_data[start_idx:end_idx]\n",
    "\n",
    "        # Reshape each source's sample into a new dimension\n",
    "        x_sample = x_source_data.reshape(samples_per_source, 180, 360, 1)\n",
    "        y_sample = y_source_data.reshape(samples_per_source, 180, 360, 1)\n",
    "\n",
    "        # Split each sample into input and output sequences\n",
    "        for j in range(samples_per_source - input_length - forecast_horizon + 1):\n",
    "            X.append(x_sample[j:j+input_length])\n",
    "            y.append(y_sample[j+input_length:j+input_length+forecast_horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def restore_original_scale(scaled_data, min_vals, max_vals):\n",
    "\n",
    "    restored_data = scaled_data * (max_vals - min_vals) + min_vals\n",
    "    return restored_data\n",
    "\n",
    "\n",
    "# Define input and output lengths\n",
    "input_length = 3\n",
    "forecast_horizon = 3\n",
    "\n",
    "# Calculate the number of sources\n",
    "num_of_sources = len(x_train_scaled) // 1040\n",
    "\n",
    "# Prepare ConvLSTM training dataset\n",
    "x_convLSTM_train, y_convLSTM_train = prepare_convLSTM_data(x_train_scaled, y_train_scaled, num_of_sources, input_length, forecast_horizon)\n",
    "np.save(f'{out_dir}/x_convLSTM_train.npy', x_convLSTM_train)\n",
    "np.save(f'{out_dir}/y_convLSTM_train.npy', y_convLSTM_train)\n",
    "\n",
    "\n",
    "# Prepare ConvLSTM testing dataset\n",
    "x_convLSTM_test, y_convLSTM_test = prepare_convLSTM_data(x_test_scaled, y_test_scaled, 6, input_length, forecast_horizon)\n",
    "np.save(f'{out_dir}/x_convLSTM_test.npy', x_convLSTM_test)\n",
    "np.save(f'{out_dir}/y_convLSTM_test.npy', y_convLSTM_test)\n",
    "\n",
    "\n",
    "min_vals, max_vals = prepare_convLSTM_data(min_vals, max_vals, 6, input_length, forecast_horizon)\n",
    "np.save(f'{out_dir}/min_vals_convlstm.npy', min_vals)\n",
    "np.save(f'{out_dir}/max_vals_convlstm.npy', max_vals)\n",
    "\n",
    "\n",
    "_, y_convLSTM_test_noscale = prepare_convLSTM_data(x_test, y_test, 6, input_length, forecast_horizon)\n",
    "np.save('/content/drive/MyDrive/Physics/Dataset10/y_convLSTM_test_3D.npy', y_convLSTM_test_noscale)\n",
    "\n",
    "y_convLSTM_test_restored = restore_original_scale(y_convLSTM_test, min_vals, max_vals)\n",
    "np.save(f'{out_dir}/y_convLSTM_test_restored.npy', y_convLSTM_test_restored)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_convLSTM_train_scaled = np.load(f'{out_dir}/x_convLSTM_train.npy', mmap_mode='r')\n",
    "y_convLSTM_train_scaled = np.load(f'{out_dir}/y_convLSTM_train.npy', mmap_mode='r')\n",
    "x_convLSTM_test_scaled = np.load(f'{out_dir}/x_convLSTM_test.npy', mmap_mode='r')\n",
    "y_convLSTM_test_scaled = np.load(f'{out_dir}/y_convLSTM_test.npy', mmap_mode='r')\n",
    "y_convLSTM_test = np.load(f'{out_dir}/y_convLSTM_test_3D.npy', mmap_mode='r')\n",
    "\n",
    "min_vals_convlstm = np.load(f'{out_dir}/min_vals_convlstm.npy', mmap_mode='r')\n",
    "max_vals_convlstm = np.load(f'{out_dir}/max_vals_convlstm.npy', mmap_mode='r')\n",
    "\n",
    "y_convLSTM_test_restored = np.load(f'{out_dir}/y_convLSTM_test_restored.npy', mmap_mode='r')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape and model architecture\n",
    "input_shape = (None, 3, 180, 360, 1)\n",
    "inputs = layers.Input(shape=input_shape[1:])\n",
    "\n",
    "# ConvLSTM layers\n",
    "x = layers.ConvLSTM2D(32, (7, 7), padding=\"same\", return_sequences=True, activation=\"relu\")(inputs)\n",
    "x = layers.ConvLSTM2D(32, (7, 7), padding=\"same\", return_sequences=True, activation=\"relu\")(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Conv3D(1, (7, 7, 7), activation=\"linear\", padding=\"same\")(x)\n",
    "\n",
    "# Model setup\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/content/drive/MyDrive/Physics/NOAA/'  # Directory for saving model\n",
    "\n",
    "# Setup callbacks for saving model, reducing learning rate, and early stopping\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=f'{save_dir}ConvLSTM-Origin-NOAA.h5', monitor='val_loss',\n",
    "    save_best_only=True, verbose=1, mode='min', save_format='h5'\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=30, min_lr=1e-5\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', patience=45, verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_convLSTM_train_scaled, y_convLSTM_train_scaled, epochs=100, batch_size=4,\n",
    "    callbacks=[reduce_lr, early_stop, model_checkpoint],\n",
    "    validation_data=(x_convLSTM_test_scaled, y_convLSTM_test_scaled), shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(f'{save_dir}best_model_convlstm.h5')\n",
    "res_lstm = model.predict(x_convLSTM_test_scaled)\n",
    "res_lstm_restored = restore_original_scale(res_lstm, min_vals_convlstm, max_vals_convlstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lstm_conv = res_lstm_restored[:,0,:,:,:]\n",
    "\n",
    "true_state = y_convLSTM_test[:,0,:,:,:]\n",
    "\n",
    "# Initialize lists to store results\n",
    "ssim_scores = []\n",
    "psnr_scores = []\n",
    "inference_times = []\n",
    "\n",
    "# Define the number of cases\n",
    "num_cases = 6\n",
    "\n",
    "# Calculate the size of each case\n",
    "case_size = len(res_lstm) // num_cases\n",
    "def apply_mask(original_images, predicted_images):\n",
    "    masked_predicted_images = []\n",
    "    for original, predicted in zip(original_images, predicted_images):\n",
    "        mask = original == 0\n",
    "\n",
    "        masked_predicted = np.where(mask, 0, predicted)\n",
    "        masked_predicted_images.append(masked_predicted)\n",
    "\n",
    "    return masked_predicted_images\n",
    "# Loop over each case\n",
    "for i in range(num_cases):\n",
    "    # Get the subset of data for the current case\n",
    "    res_subset = res_lstm_conv[i * case_size : (i + 1) * case_size]\n",
    "    true_subset = true_state[i * case_size : (i + 1) * case_size]\n",
    "    res_subset = apply_mask(true_subset, res_subset)\n",
    "    # Initialize lists to store metrics for the current case\n",
    "    case_ssim_scores = []\n",
    "    case_psnr_scores = []\n",
    "    case_inference_times = []\n",
    "\n",
    "    # Calculate metrics for each sample in the current case\n",
    "    for res, true in zip(res_subset, true_subset):\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Calculate SSIM\n",
    "        ssim_score = ssim(res, true, multichannel=True)\n",
    "        case_ssim_scores.append(ssim_score)\n",
    "\n",
    "        # Calculate PSNR\n",
    "        psnr_score = psnr(res, true, data_range=res.max() - res.min())\n",
    "        case_psnr_scores.append(psnr_score)\n",
    "\n",
    "        # Measure inference time\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        case_inference_times.append(inference_time)\n",
    "\n",
    "    # Calculate the average SSIM, PSNR, and inference time for the current case\n",
    "    avg_ssim_score = sum(case_ssim_scores) / len(case_ssim_scores)\n",
    "    avg_psnr_score = sum(case_psnr_scores) / len(case_psnr_scores)\n",
    "    avg_inference_time = sum(case_inference_times) / len(case_inference_times)\n",
    "\n",
    "    # Append the results to the lists\n",
    "    ssim_scores.append(avg_ssim_score)\n",
    "    psnr_scores.append(avg_psnr_score)\n",
    "    inference_times.append(avg_inference_time)\n",
    "\n",
    "# Print the results for each case\n",
    "for i in range(num_cases):\n",
    "    print(f\"Case {i+1}:\")\n",
    "    print(f\"Average SSIM: {ssim_scores[i]}\")\n",
    "    print(f\"Average PSNR: {psnr_scores[i]}\")\n",
    "    print(f\"Average Inference Time: {inference_times[i]} seconds\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ssim = sum(avg_ssim_score) / len(avg_ssim_score)\n",
    "\n",
    "# Calculate average PSNR\n",
    "average_psnr = sum(avg_psnr_score) / len(avg_psnr_score)\n",
    "\n",
    "# Print average values\n",
    "print(\"Average SSIM:\", average_ssim)\n",
    "print(\"Average PSNR:\", average_psnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "num_sensors = [200, 240, 280, 300, 320, 340]\n",
    "ssim = [\n",
    "    0.7237781062141283,\n",
    "    0.7539283448007513,\n",
    "    0.7524165386144361,\n",
    "    0.7638430242366332,\n",
    "    0.7577072242956256,\n",
    "    0.7650417976687304\n",
    "]\n",
    "\n",
    "# PSNR values from each case\n",
    "psnr = [\n",
    "    27.63017235612781,\n",
    "    29.54727918068748,\n",
    "    29.667194725218977,\n",
    "    30.49954460895071,\n",
    "    29.44647984272573,\n",
    "    30.73008736135083\n",
    "]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5)) \n",
    "ax1.plot(num_sensors, ssim, color='blue', marker='o', label='SSIM')\n",
    "ax1.set_xlabel('Number of Sensors', fontsize=14) \n",
    "ax1.set_ylabel('SSIM', color='blue', fontsize=14) \n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.tick_params(axis='both', labelsize=12) \n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(num_sensors, psnr, color='orange', marker='o', label='PSNR')\n",
    "ax2.set_ylabel('PSNR', color='orange', fontsize=14) \n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "ax2.tick_params(axis='both', labelsize=12) \n",
    "\n",
    "plt.title('SSIM and PSNR vs. Number of Sensors on Test Datasets', fontsize=16)  \n",
    "\n",
    "fig.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
