{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import random as rand\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import cm\n",
    "from matplotlib import animation as animation\n",
    "import matplotlib as mpl\n",
    "\n",
    "from PIL import Image\n",
    "from pylab import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Conv2D, Concatenate\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSOVT (COnvLSTM) - Basic Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vor_train = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/vor_train.npy', mmap_mode=\"r\")[:30*300]\n",
    "true_train = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/true_train.npy', mmap_mode=\"r\")[:30*300]\n",
    "vor_test = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/vor_test.npy', mmap_mode=\"r\")\n",
    "true_test = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/true_test.npy', mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_activation(x):\n",
    "    # Adjust the slicing for 5D tensors\n",
    "    tanh_activation = tf.keras.activations.tanh(x[:, :, :, :, :2])  # Apply tanh to the first two channels\n",
    "    scaled_channel_3 = tf.keras.activations.relu(x[:, :, :, :, 2:3])  # Apply ReLU to the third channel, maintaining the last dimension\n",
    "    return tf.concat([tanh_activation, scaled_channel_3], axis=-1)  # Concatenate along the channel dimension\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (None, 5, 64, 64, 3)\n",
    "\n",
    "# Input layer\n",
    "inputs = layers.Input(shape=input_shape[1:])\n",
    "\n",
    "# ConvLSTM layers\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(inputs)\n",
    "\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Conv3D(\n",
    "    filters=3,\n",
    "    kernel_size=(3, 3, 3),\n",
    "    activation=custom_activation,\n",
    "    padding=\"same\"\n",
    ")(x)\n",
    "\n",
    "# Create the model\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.mean_squared_error, optimizer=tf.keras.optimizers.Adam(), run_eagerly=True)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to save the best model\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='/content/drive/MyDrive/Physics/Physics/ConvLSTM-SW/best_model_convlstm_SW',#64_true.h5\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    save_format='h5'\n",
    ")\n",
    "\n",
    "\n",
    "# Callbacks for adaptive learning rate, early stopping, and model checkpointing\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=25,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Training the autoencoder\n",
    "history = model.fit(\n",
    "    x_convLSTM_train,\n",
    "    y_convLSTM_train,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    callbacks=[reduce_lr, model_checkpoint],  # Include model_checkpoint in the callbacks\n",
    "    validation_data=(x_convLSTM_test, y_convLSTM_test),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with custom activation function\n",
    "model = tf.keras.models.load_model('./Model_para/ConvLSTM/ConvLSTM-Origin-SW', custom_objects={'custom_activation': custom_activation})\n",
    "start_time = time.time()\n",
    "res_lstm = model.predict(x_convLSTM_test)\n",
    "\n",
    "prediction_end_time = time.time()\n",
    "\n",
    "print(\"Time taken for prediction:\", prediction_end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./Model_para/ConvLSTM/ConvLSTM-Origin-SW', custom_objects={'custom_activation': custom_activation})\n",
    "\n",
    "# Predict using the model\n",
    "start_time = time.time()\n",
    "res_lstm = model.predict(x_convLSTM_test)\n",
    "prediction_end_time = time.time()\n",
    "\n",
    "print(\"Time taken for prediction:\", prediction_end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "ssim_values = []\n",
    "psnr_values = []\n",
    "\n",
    "for i in range(len(y_convLSTM_test[:,0,:,:,:])):\n",
    "    ssim_val = ssim(y_convLSTM_test[:,0,:,:,:][i], res_lstm[:,0,:,:,:][i], multichannel=True)\n",
    "    psnr_val = psnr(y_convLSTM_test[:,0,:,:,:][i], res_lstm[:,0,:,:,:][i], data_range=2)\n",
    "\n",
    "    ssim_values.append(ssim_val)\n",
    "    psnr_values.append(psnr_val)\n",
    "\n",
    "average_ssim = sum(ssim_values) / len(ssim_values)\n",
    "average_psnr = sum(psnr_values) / len(psnr_values)\n",
    "\n",
    "print(f\"Average SSIM: {average_ssim}\")\n",
    "print(f\"Average PSNR: {average_psnr}\")\n",
    "print(f\"Average PSNR: {calculate_rrmse(y_convLSTM_test,res_lstm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSOVT (COnvLSTM) - Physics Constrained Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vor_train = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/vor_train.npy', mmap_mode=\"r\")[:10*300]\n",
    "true_train = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/true_train.npy', mmap_mode=\"r\")[:10*300]\n",
    "vor_test = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/vor_test.npy', mmap_mode=\"r\")\n",
    "true_test = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/true_test.npy', mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-Constrained ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_energy(img):\n",
    "    g = 9.81\n",
    "    dx = 1\n",
    "    img = tf.reshape(img, (-1, img.shape[2], img.shape[3],3))\n",
    "\n",
    "    kinetic_energy = 0.5 * tf.reduce_sum(tf.square(img[..., 0]), axis=(1, 2)) * dx**2\n",
    "    kinetic_energy += 0.5 * tf.reduce_sum(tf.square(img[..., 1]), axis=(1, 2)) * dx**2\n",
    "    # Calculate potential energy\n",
    "    potential_energy = 0.5 * g * tf.reduce_sum(tf.square(img[..., 2]), axis=(1, 2)) * dx**2\n",
    "    # Calculate total energy\n",
    "    total_energy = kinetic_energy + potential_energy\n",
    "    return total_energy\n",
    "\n",
    "def calculate_mass_conservation(y):\n",
    "    y = tf.reshape(y, (-1, y.shape[2], y.shape[3], 3))\n",
    "    h, u, v = y[..., 2:3], y[..., 0:1], y[..., 1:2]\n",
    "    dh_dx, dh_dy = tf.image.image_gradients(h)\n",
    "    du_dx, _ = tf.image.image_gradients(u)\n",
    "    _, dv_dy = tf.image.image_gradients(v)\n",
    "    mass_conservation = dh_dx * u + h * du_dx + dh_dy * v + h * dv_dy\n",
    "\n",
    "    mass_conservation_loss = tf.reduce_mean(tf.abs(mass_conservation), axis=[1, 2, 3])\n",
    "    return mass_conservation_loss\n",
    "\n",
    "def calculate_mass_conservation_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    mass_conservation_true = calculate_mass_conservation(y_true)\n",
    "    mass_conservation_pred = calculate_mass_conservation(y_pred)\n",
    "    return tf.reduce_mean(tf.abs(mass_conservation_pred - mass_conservation_true))\n",
    "\n",
    "def calculate_relative_energy_error(x_corr_true, y_pred):\n",
    "    energy_true = calculate_image_energy(x_corr_true)\n",
    "    energy_pred = calculate_image_energy(y_pred)\n",
    "    return tf.reduce_sum(tf.abs(energy_pred - energy_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_convLSTM_data(x_data, y_data, num_of_sources, input_length=3, forecast_horizon=3):\n",
    "    X, y = [], []\n",
    "\n",
    "    # 计算每个源的样本数量\n",
    "    samples_per_source = len(x_data) // num_of_sources\n",
    "\n",
    "    for i in range(num_of_sources):\n",
    "        # 根据每个源的样本数量计算起始和结束索引\n",
    "        start_idx = i * samples_per_source\n",
    "        end_idx = start_idx + samples_per_source\n",
    "\n",
    "        # 切分x_data和y_data，获取当前源的数据\n",
    "        x_source_data = x_data[start_idx:end_idx]\n",
    "        y_source_data = y_data[start_idx:end_idx]\n",
    "\n",
    "        # 将每个样本的源数量合并成一个新的维度\n",
    "        x_sample = x_source_data\n",
    "        y_sample = y_source_data\n",
    "\n",
    "        # 将每个样本划分为输入和输出序列\n",
    "        for j in range(samples_per_source - input_length - forecast_horizon + 1):\n",
    "            X.append(x_sample[j:j+input_length])\n",
    "            y.append(y_sample[j+input_length:j+input_length+forecast_horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "input_length = 5\n",
    "forecast_horizon = 5\n",
    "\n",
    "# 计算源的数量\n",
    "num_of_sources = len(vor_train) // 300\n",
    "x_convLSTM_train, y_convLSTM_train = prepare_convLSTM_data(vor_train, true_train, num_of_sources, input_length, forecast_horizon)\n",
    "x_convLSTM_test, y_convLSTM_test = prepare_convLSTM_data(vor_test, true_test, 10, input_length, forecast_horizon)\n",
    "x_corr_true_train,_ = prepare_convLSTM_data(true_train,true_train, num_of_sources, input_length, forecast_horizon)\n",
    "x_corr_true_test,_ = prepare_convLSTM_data(true_test,true_test, 10, input_length, forecast_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_energy(img):\n",
    "    g = tf.constant(9.81, dtype=tf.float32)  # Ensure g is float32\n",
    "    dx = tf.constant(1, dtype=tf.float32)  # Ensure dx is float32\n",
    "    img = tf.cast(img, tf.float32)  # Convert img to float32\n",
    "    img = tf.reshape(img, (-1, img.shape[2], img.shape[3], 3))\n",
    "\n",
    "    kinetic_energy = 0.5 * tf.reduce_sum(tf.square(img[..., 0]), axis=(1, 2)) * dx**2\n",
    "    kinetic_energy += 0.5 * tf.reduce_sum(tf.square(img[..., 1]), axis=(1, 2)) * dx**2\n",
    "    potential_energy = 0.5 * g * tf.reduce_sum(tf.square(img[..., 2]), axis=(1, 2)) * dx**2\n",
    "    total_energy = kinetic_energy + potential_energy\n",
    "    return total_energy\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def custom_activation(x):\n",
    "    # Adjust the slicing for 5D tensors\n",
    "    tanh_activation = tf.keras.activations.tanh(x[:, :, :, :, :2])  # Apply tanh to the first two channels\n",
    "    scaled_channel_3 = tf.keras.activations.relu(x[:, :, :, :, 2:3])  # Apply ReLU to the third channel, maintaining the last dimension\n",
    "    return tf.concat([tanh_activation, scaled_channel_3], axis=-1)  # Concatenate along the channel dimension\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (None, 5, 64, 64, 3)\n",
    "\n",
    "# Input layer\n",
    "inputs = layers.Input(shape=input_shape[1:])\n",
    "\n",
    "# ConvLSTM layers\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(inputs)\n",
    "\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Conv3D(\n",
    "    filters=3,\n",
    "    kernel_size=(3, 3, 3),\n",
    "    activation=custom_activation,\n",
    "    padding=\"same\"\n",
    ")(x)\n",
    "\n",
    "# Create the model\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_convLSTM_train, y_convLSTM_train, x_corr_true_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_convLSTM_test, y_convLSTM_test, x_corr_true_test))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Define custom loss wrapper\n",
    "def custom_loss_wrapper(input_tensor, epoch):\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        if epoch < 15:\n",
    "            # Use MSE loss for the first 20 epochs\n",
    "            mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "            #energy_restriction = calculate_relative_energy_error(input_tensor, y_pred)\n",
    "\n",
    "            total_loss = mse_loss #+ 1e-10 * energy_restriction\n",
    "            return total_loss\n",
    "        else:\n",
    "            # Switch to custom loss after 20 epochs\n",
    "            mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "            mass_conservation_loss_value = calculate_mass_conservation_loss(input_tensor, y_pred)\n",
    "            energy_restriction = calculate_relative_energy_error(input_tensor, y_pred)\n",
    "            total_loss = mse_loss + 5e-10 * energy_restriction#+0.0000001*mass_conservation_loss_value\n",
    "            return total_loss\n",
    "\n",
    "    return custom_loss\n",
    "\n",
    "best_val_mse = float('inf')  # Initialize as infinity\n",
    "best_model_path = './Model_para/ConvLSTM/ConvLSTM-Phy-SW'  # Model save path\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(70):\n",
    "    total_mse_loss_train = 0.0\n",
    "    total_steps_train = 0\n",
    "    total_mse_loss_val = 0.0\n",
    "    total_steps_val = 0\n",
    "\n",
    "    # Training\n",
    "    for step, (x_batch, y_batch, x_corr_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch, training=True)\n",
    "            # Calculate loss using custom loss wrapper\n",
    "            loss_fn = custom_loss_wrapper(x_corr_batch, epoch)\n",
    "            total_loss = loss_fn(y_batch, y_pred)\n",
    "            total_mse_loss_train += tf.reduce_mean(total_loss).numpy()\n",
    "            total_steps_train += 1\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    average_mse_loss_train = total_mse_loss_train / total_steps_train\n",
    "    print(f\"Epoch {epoch+1}, Training Average MSE Loss: {average_mse_loss_train}\")\n",
    "\n",
    "    # Validation\n",
    "    for x_batch, y_batch, x_corr_batch in val_dataset:\n",
    "        y_pred = model(x_batch, training=False)\n",
    "        # Calculate loss using custom loss wrapper\n",
    "        loss_fn = custom_loss_wrapper(x_corr_batch, epoch)\n",
    "        total_loss = loss_fn(y_batch, y_pred)\n",
    "        total_mse_loss_val += tf.reduce_mean(total_loss).numpy()\n",
    "        total_steps_val += 1\n",
    "\n",
    "    average_mse_loss_val = total_mse_loss_val / total_steps_val\n",
    "    print(f\"Epoch {epoch+1}, Validation Average MSE Loss: {average_mse_loss_val}\")\n",
    "\n",
    "    # Check for best model\n",
    "    if average_mse_loss_val < best_val_mse:\n",
    "        best_val_mse = average_mse_loss_val\n",
    "        model.save(best_model_path)  # Save the best model\n",
    "        print(f\"New best model saved at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvLSTM - Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def calculate_image_energy(img):\n",
    "    g = tf.constant(9.81, dtype=tf.float32)  # Ensure g is float32\n",
    "    dx = tf.constant(1, dtype=tf.float32)  # Ensure dx is float32\n",
    "    img = tf.cast(img, tf.float32)  # Convert img to float32\n",
    "    img = tf.reshape(img, (-1, img.shape[2], img.shape[3], 3))\n",
    "\n",
    "    kinetic_energy = 0.5 * tf.reduce_sum(tf.square(img[..., 0]), axis=(1, 2)) * dx**2\n",
    "    kinetic_energy += 0.5 * tf.reduce_sum(tf.squa kre(img[..., 1]), axis=(1, 2)) * dx**2\n",
    "    potential_energy = 0.5 * g * tf.reduce_sum(tf.square(img[..., 2]), axis=(1, 2)) * dx**2\n",
    "    total_energy = kinetic_energy + potential_energy\n",
    "    return total_energy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def custom_activation(x):\n",
    "    # Adjust the slicing for 5D tensors\n",
    "    tanh_activation = tf.keras.activations.tanh(x[:, :, :, :, :2])  # Apply tanh to the first two channels\n",
    "    scaled_channel_3 = tf.keras.activations.relu(x[:, :, :, :, 2:3])  # Apply ReLU to the third channel, maintaining the last dimension\n",
    "    return tf.concat([tanh_activation, scaled_channel_3], axis=-1)  # Concatenate along the channel dimension\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (None, 5, 64, 64, 3)\n",
    "\n",
    "# Input layer\n",
    "inputs = layers.Input(shape=input_shape[1:])\n",
    "\n",
    "# ConvLSTM layers\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(inputs)\n",
    "\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Conv3D(\n",
    "    filters=3,\n",
    "    kernel_size=(3, 3, 3),\n",
    "    activation=custom_activation,\n",
    "    padding=\"same\"\n",
    ")(x)\n",
    "\n",
    "# Create the model\n",
    "orig_model = models.Model(inputs, outputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_convLSTM_train, y_convLSTM_train, x_corr_true_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_convLSTM_test, y_convLSTM_test, x_corr_true_test))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Define custom loss wrapper\n",
    "def custom_loss_wrapper(input_tensor, epoch):\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        if epoch < 100:\n",
    "            # Use MSE loss for the first 20 epochs\n",
    "            mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "            energy_restriction = calculate_relative_energy_error(input_tensor, y_pred)\n",
    "            total_loss = mse_loss #+ 1e-10 * energy_restriction\n",
    "            return total_loss\n",
    "        else:\n",
    "            # Switch to custom loss after 20 epochs\n",
    "            mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "            energy_restriction = calculate_relative_energy_error(input_tensor, y_pred)\n",
    "            total_loss = mse_loss + 6e-10 * energy_restriction\n",
    "            return total_loss\n",
    "\n",
    "    return custom_loss\n",
    "\n",
    "best_val_mse = float('inf')  # Initialize as infinity\n",
    "best_model_path = './Model_para/ConvLSTM/ConvLSTM-BP-SW'  # Model save path\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create the model\n",
    "orig_model = models.Model(inputs, outputs)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=25,          # Stop training if no improvement after 25 epochs\n",
    "    restore_best_weights=True  # Restore weights to the best model\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    total_mse_loss_train = 0.0\n",
    "    total_steps_train = 0\n",
    "    total_mse_loss_val = 0.0\n",
    "    total_steps_val = 0\n",
    "\n",
    "    # Training\n",
    "    for step, (x_batch, y_batch, x_corr_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = orig_model(x_batch, training=True)\n",
    "            # Calculate loss using custom loss wrapper\n",
    "            loss_fn = custom_loss_wrapper(x_corr_batch, epoch)\n",
    "            total_loss = loss_fn(y_batch, y_pred)\n",
    "            total_mse_loss_train += tf.reduce_mean(total_loss).numpy()\n",
    "            total_steps_train += 1\n",
    "        grads = tape.gradient(total_loss, orig_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, orig_model.trainable_variables))\n",
    "\n",
    "    average_mse_loss_train = total_mse_loss_train / total_steps_train\n",
    "    print(f\"Epoch {epoch+1}, Training Average MSE Loss: {average_mse_loss_train}\")\n",
    "\n",
    "    # Validation\n",
    "    for x_batch, y_batch, x_corr_batch in val_dataset:\n",
    "        y_pred = orig_model(x_batch, training=False)\n",
    "        # Calculate loss using custom loss wrapper\n",
    "        loss_fn = custom_loss_wrapper(x_corr_batch, epoch)\n",
    "        total_loss = loss_fn(y_batch, y_pred)\n",
    "        total_mse_loss_val += tf.reduce_mean(total_loss).numpy()\n",
    "        total_steps_val += 1\n",
    "\n",
    "    average_mse_loss_val = total_mse_loss_val / total_steps_val\n",
    "    print(f\"Epoch {epoch+1}, Validation Average MSE Loss: {average_mse_loss_val}\")\n",
    "\n",
    "    # Check for best model\n",
    "    if average_mse_loss_val < best_val_mse:\n",
    "        best_val_mse = average_mse_loss_val\n",
    "        orig_model.save(best_model_path)  # Save the best model\n",
    "        print(f\"New best model saved at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = tf.keras.models.load_model(\"./Model_para/ConvLSTM/ConvLSTM-BP-SW\")\n",
    "model = tf.keras.models.load_model(\"./Model_para/ConvLSTM/ConvLSTM-Physics-SW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "reshaped_features = vor_test.reshape((10, 300, 64,64,3))\n",
    "\n",
    "# Extract features for original model\n",
    "extracted_features_batches = []\n",
    "x = 5\n",
    "for feature_batch in reshaped_features:\n",
    "    extracted_batch = np.array([feature_batch[i:i+x] for i in range(0, feature_batch.shape[0] - 10, x)])\n",
    "    extracted_features_batches.append(extracted_batch)\n",
    "extracted_features_batches = np.array(extracted_features_batches)\n",
    "\n",
    "# Extract true images for physics-based model\n",
    "reshaped_features = true_test.reshape((10, 300, 64, 64, 3))\n",
    "extracted_true_image = []\n",
    "x = 5\n",
    "for feature_batch in reshaped_features:\n",
    "    extracted_batch = np.array([feature_batch[i:i+x] for i in range(10, feature_batch.shape[0], x)])\n",
    "    extracted_true_image.append(extracted_batch)\n",
    "extracted_true_image = np.array(extracted_true_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rrmse(true, predicted):\n",
    "    # Calculate the Root Mean Square Error (RMSE)\n",
    "    rmse = np.sqrt(np.mean((true - predicted) ** 2))\n",
    "\n",
    "    # Calculate the mean of the true data\n",
    "    mean_true = np.mean(true)\n",
    "\n",
    "    # Avoid division by zero by handling the case where the mean of the true data is zero\n",
    "    if mean_true == 0:\n",
    "        return np.inf\n",
    "\n",
    "    # Calculate the Relative Root Mean Square Error (R-RMSE)\n",
    "    rrmse = rmse / mean_true\n",
    "    return rrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define starting position, step size, and sequence length\n",
    "START_POSITION = 15\n",
    "STEP_SIZE = 5\n",
    "\n",
    "\n",
    "def rolling_forecast(model, initial_input, true_images):\n",
    "    input_sequence = initial_input.reshape(1, STEP_SIZE, 64,64,3)  # Reshape to match LSTM input shape\n",
    "    predictions = []\n",
    "    mse_array = []\n",
    "    true_images_array = []\n",
    "    for i in range(32):  # Consider 5 steps at a time\n",
    "\n",
    "        next_steps = model.predict(input_sequence)\n",
    "        predicted_image = next_steps.reshape(-1, 64,64,3)\n",
    "\n",
    "        true_image = true_images[START_POSITION + i]\n",
    "\n",
    "        predictions.extend(predicted_image)\n",
    "        mse = calculate_rrmse(true_image,predicted_image).reshape(-1, 1)#np.mean((predicted_image - true_image) ** 2, axis=(1, 2, 3)).reshape(-1, 1)\n",
    "\n",
    "        mse_array.append(mse)\n",
    "        true_images_array.append(true_image)\n",
    "        input_sequence = next_steps\n",
    "    return np.array(predictions), np.array(mse_array), np.array(true_images_array)\n",
    "\n",
    "def prepare_data(features_batches, true_image_batches, batch_index):\n",
    "    return features_batches[batch_index], true_image_batches[batch_index]\n",
    "\n",
    "n = 1\n",
    "features_batches, true_image_batches = extracted_features_batches[n:(n+9)], extracted_true_image[n:(n+9)]\n",
    "\n",
    "all_mse_orig = []\n",
    "all_mse_phy = []\n",
    "pred_orig_list = []\n",
    "pred_phy_list = []\n",
    "hist_mse_orig = []\n",
    "hist_mse_phy = []\n",
    "true_orig_list = []  # Renamed from t_orig_list\n",
    "true_phy_list = []\n",
    "\n",
    "# Collect MSE for each data set\n",
    "for i in range(9):\n",
    "    x_test_cu, y_test_cu = prepare_data(features_batches, true_image_batches, i)\n",
    "    initial_input = x_test_cu[START_POSITION, :, :]\n",
    "\n",
    "    pred_orig, mse_orig, true_orig = rolling_forecast(orig_model, initial_input, y_test_cu)\n",
    "    pred_phy, mse_phy, true_phy = rolling_forecast(model, initial_input, y_test_cu)\n",
    "\n",
    "    avg_mse_per_group_orig = np.mean(mse_orig, axis=1)\n",
    "    avg_mse_per_group_phy = np.mean(mse_phy, axis=1)\n",
    "\n",
    "    hist_mse_orig.append(mse_orig)\n",
    "    hist_mse_phy.append(mse_phy)\n",
    "    all_mse_orig.append(avg_mse_per_group_orig)\n",
    "    all_mse_phy.append(avg_mse_per_group_phy)\n",
    "\n",
    "    pred_orig_list.append(pred_orig)\n",
    "    pred_phy_list.append(pred_phy)\n",
    "    true_orig_list.append(true_orig)\n",
    "    true_phy_list.append(true_phy)\n",
    "\n",
    "all_mse_orig = np.array(all_mse_orig)\n",
    "all_mse_phy = np.array(all_mse_phy)\n",
    "\n",
    "# Filter out groups with maximum MSE below the threshold\n",
    "threshold = 1\n",
    "is_below_threshold_orig = np.max(all_mse_orig, axis=(1, 2)) <= threshold\n",
    "filtered_mse_orig = all_mse_orig[is_below_threshold_orig]\n",
    "\n",
    "is_below_threshold_phy = np.max(all_mse_phy, axis=(1, 2)) <= threshold\n",
    "filtered_mse_phy = all_mse_phy[is_below_threshold_phy]\n",
    "\n",
    "# Calculate average and standard deviation of filtered MSE\n",
    "avg_mse_orig = np.mean(filtered_mse_orig, axis=0)\n",
    "std_mse_orig = np.std(filtered_mse_orig, axis=0)\n",
    "\n",
    "avg_mse_phy = np.mean(filtered_mse_phy, axis=0)\n",
    "std_mse_phy = np.std(filtered_mse_phy, axis=0)\n",
    "\n",
    "# Flatten arrays for plotting\n",
    "avg_mse_orig = np.asarray(avg_mse_orig).flatten()\n",
    "std_mse_orig = np.asarray(std_mse_orig).flatten()\n",
    "avg_mse_phy = np.asarray(avg_mse_phy).flatten()\n",
    "std_mse_phy = np.asarray(std_mse_phy).flatten()\n",
    "\n",
    "# Plot aggregated MSE with standard deviation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(avg_mse_orig, label='Original Model Average MSE')\n",
    "plt.fill_between(range(len(avg_mse_orig)),\n",
    "                 avg_mse_orig - std_mse_orig,\n",
    "                 avg_mse_orig + std_mse_orig,\n",
    "                 alpha=0.2)\n",
    "plt.plot(avg_mse_phy, label='Physics-based Model Average MSE')\n",
    "plt.fill_between(range(len(avg_mse_phy)), avg_mse_phy - std_mse_phy, avg_mse_phy + std_mse_phy, alpha=0.2)\n",
    "\n",
    "plt.title('Aggregated Images MSE with Standard Deviation over Steps Compared to Decoded True Fields')\n",
    "plt.xlabel('Step', fontsize=16)\n",
    "plt.ylabel('MSE', fontsize=16)\n",
    "plt.legend(loc='upper right', fontsize=9)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mse_orig = np.array(hist_mse_orig).reshape((-1,1))\n",
    "hist_mse_phy = np.array(hist_mse_phy).reshape((-1,1))\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "color_orig_hist = 'skyblue'\n",
    "color_phy_hist = 'orange'\n",
    "edge_color = 'black'\n",
    "plt.hist(hist_mse_orig, bins=30, alpha=0.75, color='skyblue', edgecolor='black', label='Basic ConvLSTM Model')\n",
    "plt.hist(hist_mse_phy, bins=30, alpha=0.75, color='orange', edgecolor='black', label='Physics-based ConLSTM Model')\n",
    "\n",
    "plt.xlabel('Error Value', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Histogram of MSE Values for Two Models', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Define starting position, step size, and sequence length\n",
    "START_POSITION = 15\n",
    "STEP_SIZE = 5\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "def rolling_forecast(model, initial_input, true_images):\n",
    "    input_sequence = initial_input.reshape(1, STEP_SIZE, 64,64,3)  # Reshape to match LSTM input shape\n",
    "    predictions = []\n",
    "    psnr_array = []\n",
    "    true_images_array = []\n",
    "    for i in range(32):  # Consider 5 steps at a time\n",
    "        next_steps = model.predict(input_sequence)\n",
    "        predicted_image = next_steps.reshape(-1, 64,64,3)\n",
    "\n",
    "        true_image = true_images[START_POSITION + i]\n",
    "\n",
    "        # Convert TensorFlow tensor to NumPy array\n",
    "        predicted_image_np = np.array(predicted_image)\n",
    "\n",
    "        predictions.extend(predicted_image)\n",
    "        psnr_val = psnr(true_image, predicted_image_np, data_range=predicted_image_np.max() - predicted_image_np.min())\n",
    "        psnr_array.append([psnr_val])\n",
    "        true_images_array.append(true_image)\n",
    "        input_sequence = next_steps\n",
    "    return np.array(predictions), np.array(psnr_array), np.array(true_images_array)\n",
    "\n",
    "\n",
    "def prepare_data(features_batches, true_image_batches, batch_index):\n",
    "    return features_batches[batch_index], true_image_batches[batch_index]\n",
    "\n",
    "n = 1\n",
    "features_batches, true_image_batches = extracted_features_batches[n:(n+9)], extracted_true_image[n:(n+9)]\n",
    "\n",
    "all_psnr_orig = []\n",
    "all_psnr_phy = []\n",
    "pred_orig_list = []\n",
    "pred_phy_list = []\n",
    "\n",
    "true_orig_list = []  # Renamed from t_orig_list\n",
    "true_phy_list = []  # Renamed from t_phy_list\n",
    "\n",
    "# Collect PSNR for each data set\n",
    "for i in range(9):\n",
    "    x_test_cu, y_test_cu = prepare_data(features_batches, true_image_batches, i)\n",
    "    initial_input = x_test_cu[START_POSITION, :, :]\n",
    "\n",
    "    pred_orig, psnr_orig, true_orig = rolling_forecast(orig_model, initial_input, y_test_cu)\n",
    "    pred_phy, psnr_phy, true_phy = rolling_forecast(model, initial_input, y_test_cu)\n",
    "\n",
    "    avg_psnr_per_group_orig = np.mean(psnr_orig, axis=1)\n",
    "    avg_psnr_per_group_phy = np.mean(psnr_phy, axis=1)\n",
    "\n",
    "    all_psnr_orig.append(avg_psnr_per_group_orig)\n",
    "    all_psnr_phy.append(avg_psnr_per_group_phy)\n",
    "\n",
    "    pred_orig_list.append(pred_orig)\n",
    "    pred_phy_list.append(pred_phy)\n",
    "    true_orig_list.append(true_orig)\n",
    "    true_phy_list.append(true_phy)\n",
    "\n",
    "all_psnr_orig = np.array(all_psnr_orig)\n",
    "all_psnr_phy = np.array(all_psnr_phy)\n",
    "\n",
    "filtered_psnr_orig = all_psnr_orig\n",
    "filtered_psnr_phy = all_psnr_phy\n",
    "\n",
    "# Calculate average and standard deviation of filtered PSNR\n",
    "avg_psnr_orig = np.mean(filtered_psnr_orig, axis=0)\n",
    "std_psnr_orig = np.std(filtered_psnr_orig, axis=0)\n",
    "\n",
    "avg_psnr_phy = np.mean(filtered_psnr_phy, axis=0)\n",
    "std_psnr_phy = np.std(filtered_psnr_phy, axis=0)\n",
    "\n",
    "# Flatten arrays for plotting\n",
    "avg_psnr_orig = np.asarray(avg_psnr_orig).flatten()\n",
    "std_psnr_orig = np.asarray(std_psnr_orig).flatten()\n",
    "avg_psnr_phy = np.asarray(avg_psnr_phy).flatten()\n",
    "std_psnr_phy = np.asarray(std_psnr_phy).flatten()\n",
    "\n",
    "# Plot aggregated PSNR with standard deviation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(avg_psnr_orig, label='Original Model Average PSNR')\n",
    "plt.fill_between(range(len(avg_psnr_orig)),\n",
    "                 avg_psnr_orig - std_psnr_orig,\n",
    "                 avg_psnr_orig + std_psnr_orig,\n",
    "                 alpha=0.2)\n",
    "plt.plot(avg_psnr_phy, label='Physics-based Model Average PSNR')\n",
    "plt.fill_between(range(len(avg_psnr_phy)), avg_psnr_phy - std_psnr_phy, avg_psnr_phy + std_psnr_phy, alpha=0.2)\n",
    "\n",
    "plt.title('Aggregated Images PSNR with Standard Deviation over Steps Compared to True Fields')\n",
    "plt.xlabel('Step', fontsize=16)\n",
    "plt.ylabel('PSNR', fontsize=16)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Define starting position, step size, and sequence length\n",
    "START_POSITION = 15\n",
    "STEP_SIZE = 5\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "def rolling_forecast(model, initial_input, true_images):\n",
    "    input_sequence = initial_input.reshape(1, STEP_SIZE, 64,64,3)  # Reshape to match LSTM input shape\n",
    "    predictions = []\n",
    "    ssim_array = []\n",
    "    true_images_array = []\n",
    "    for i in range(32):  # Consider 5 steps at a time\n",
    "        next_steps = model.predict(input_sequence)\n",
    "        predicted_image = next_steps.reshape(-1, 64,64,3)\n",
    "        true_image = true_images[START_POSITION + i]\n",
    "\n",
    "        # Convert TensorFlow tensor to NumPy array\n",
    "        predicted_image_np = np.array(predicted_image)\n",
    "\n",
    "        predictions.extend(predicted_image)\n",
    "        ssim_val, _ = ssim(predicted_image_np, true_image, multichannel=True, full=True, win_size=5)\n",
    "        ssim_array.append([ssim_val])\n",
    "        true_images_array.append(true_image)\n",
    "        input_sequence = next_steps\n",
    "    return np.array(predictions), np.array(ssim_array), np.array(true_images_array)\n",
    "\n",
    "def prepare_data(features_batches, true_image_batches, batch_index):\n",
    "    return features_batches[batch_index], true_image_batches[batch_index]\n",
    "\n",
    "n = 1\n",
    "features_batches, true_image_batches = extracted_features_batches[n:(n+9)], extracted_true_image[n:(n+9)]\n",
    "\n",
    "all_ssim_orig = []\n",
    "all_ssim_phy = []\n",
    "pred_orig_list = []\n",
    "pred_phy_list = []\n",
    "\n",
    "true_orig_list = []  # Renamed from t_orig_list\n",
    "true_phy_list = []  # Renamed from t_phy_list\n",
    "\n",
    "# Collect SSIM for each data set\n",
    "for i in range(9):\n",
    "    x_test_cu, y_test_cu = prepare_data(features_batches, true_image_batches, i)\n",
    "    initial_input = x_test_cu[START_POSITION, :, :]\n",
    "\n",
    "    pred_orig, ssim_orig, true_orig = rolling_forecast(orig_model, initial_input, y_test_cu)\n",
    "    pred_phy, ssim_phy, true_phy = rolling_forecast(model, initial_input, y_test_cu)\n",
    "\n",
    "    avg_ssim_per_group_orig = np.mean(ssim_orig, axis=1)\n",
    "    avg_ssim_per_group_phy = np.mean(ssim_phy, axis=1)\n",
    "\n",
    "    all_ssim_orig.append(avg_ssim_per_group_orig)\n",
    "    all_ssim_phy.append(avg_ssim_per_group_phy)\n",
    "\n",
    "    pred_orig_list.append(pred_orig)\n",
    "    pred_phy_list.append(pred_phy)\n",
    "    true_orig_list.append(true_orig)\n",
    "    true_phy_list.append(true_phy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Define color palette\n",
    "color_orig = 'blue'\n",
    "color_phy = 'red'\n",
    "color_fill_orig = 'lightblue'\n",
    "color_fill_phy = 'salmon'  # Using 'salmon' as a lighter shade of red\n",
    "\n",
    "# Plot MSE\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(avg_mse_orig, label='Basic Model', color=color_orig)\n",
    "plt.fill_between(range(len(avg_mse_orig)),\n",
    "                 avg_mse_orig - std_mse_orig,\n",
    "                 avg_mse_orig + std_mse_orig,\n",
    "                 alpha=0.2, color=color_fill_orig)\n",
    "plt.plot(avg_mse_phy, label='Physics-based Model', color=color_phy)\n",
    "plt.fill_between(range(len(avg_mse_phy)),\n",
    "                 avg_mse_phy - std_mse_phy,\n",
    "                 avg_mse_phy + std_mse_phy,\n",
    "                 alpha=0.2, color=color_fill_phy)\n",
    "plt.title('R-RMSE with Standard Deviation over Steps', fontsize=16)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('R-RMSE', fontsize=12)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot SSIM\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(avg_ssim_orig, label='Basic Model', color=color_orig)\n",
    "plt.fill_between(range(len(avg_ssim_orig)),\n",
    "                 avg_ssim_orig - std_ssim_orig,\n",
    "                 avg_ssim_orig + std_ssim_orig,\n",
    "                 alpha=0.2, color=color_fill_orig)\n",
    "plt.plot(avg_ssim_phy, label='Physics-based Model', color=color_phy)\n",
    "plt.fill_between(range(len(avg_ssim_phy)),\n",
    "                 avg_ssim_phy - std_ssim_phy,\n",
    "                 avg_ssim_phy + std_ssim_phy,\n",
    "                 alpha=0.2, color=color_fill_phy)\n",
    "plt.title('SSIM with Standard Deviation over Steps', fontsize=16)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('SSIM', fontsize=12)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot PSNR\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(avg_psnr_orig, label='Basic Model', color=color_orig)\n",
    "plt.fill_between(range(len(avg_psnr_orig)),\n",
    "                 avg_psnr_orig - std_psnr_orig,\n",
    "                 avg_psnr_orig + std_psnr_orig,\n",
    "                 alpha=0.2, color=color_fill_orig)\n",
    "plt.plot(avg_psnr_phy, label='Physics-based Model', color=color_phy)\n",
    "plt.fill_between(range(len(avg_psnr_phy)),\n",
    "                 avg_psnr_phy - std_psnr_phy,\n",
    "                 avg_psnr_phy + std_psnr_phy,\n",
    "                 alpha=0.2, color=color_fill_phy)\n",
    "plt.title('PSNR with Standard Deviation over Steps', fontsize=16)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('PSNR', fontsize=12)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
