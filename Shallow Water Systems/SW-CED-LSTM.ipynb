{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import math\n",
    "import random as rand\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import cm\n",
    "from matplotlib import animation as animation\n",
    "import matplotlib as mpl\n",
    "\n",
    "from PIL import Image\n",
    "from pylab import *\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Input, Conv2D, Concatenate\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vor_train = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/vor_train.npy', mmap_mode=\"r\")[:30*300]\n",
    "true_train = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/true_train.npy', mmap_mode=\"r\")[:30*300]\n",
    "vor_test = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/vor_test.npy', mmap_mode=\"r\")\n",
    "true_test = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/true_test.npy', mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSOVT - CED-LSTM - Basic Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(1)\n",
    "def custom_activation(x):\n",
    "    tanh_activation = tf.keras.activations.tanh(x[:, :, :, :2])\n",
    "    scaled_channel_3 = tf.keras.activations.relu(x[:, :, :, 2])\n",
    "    return tf.concat([tanh_activation, tf.expand_dims(scaled_channel_3, axis=-1)], axis=-1)\n",
    "# Encoder\n",
    "input_img = layers.Input(shape=(64, 64, 3))\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "encoded = layers.Dense(128, activation='relu', name='encoded')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Dense(16*16*64, activation='relu')(encoded)\n",
    "x = layers.Reshape((16, 16, 64))(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2D(3, (3, 3), activation=custom_activation, padding='same', name='decoded')(x)\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Callbacks for adaptive learning rate, early stopping, and model checkpoint\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=55,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=60,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    './Model_para/CED_LSTM/CED-Direct-SW-image',#notry\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    vor_train,\n",
    "    true_train,\n",
    "    epochs=200,\n",
    "    batch_size=8,\n",
    "    callbacks=[reduce_lr, early_stop, model_checkpoint],\n",
    "    validation_data=(vor_test, true_test),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vor_image = np.vstack([vor_train,vor_test])\n",
    "true_image = np.vstack([true_train,true_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.models.Model(input_img, encoded)\n",
    "\n",
    "encoded_features = encoder.predict(vor_image)\n",
    "reconstructed_images = autoencoder.predict(vor_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import numpy as np\n",
    "\n",
    "def calculate_rrmse(true, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Root Mean Square Error (R-RMSE).\n",
    "    \"\"\"\n",
    "    # Calculate the Root Mean Square Error (RMSE)\n",
    "    rmse = np.sqrt(np.mean((true - predicted) ** 2))\n",
    "\n",
    "    # Calculate the mean of the true data\n",
    "    mean_true = np.mean(true)\n",
    "\n",
    "    # Avoid division by zero by handling the case where the mean of the true data is zero\n",
    "    if mean_true == 0:\n",
    "        return np.inf\n",
    "\n",
    "    # Calculate the Relative Root Mean Square Error (R-RMSE)\n",
    "    rrmse = rmse / mean_true\n",
    "    return rrmse\n",
    "\n",
    "# Initialize SSIM and PSNR lists\n",
    "ssim_values = []\n",
    "psnr_values = []\n",
    "\n",
    "# Calculate SSIM and PSNR for each pair of images\n",
    "for i in range(len(true_test)):\n",
    "    ssim_val = ssim(true_test[i], reconstructed_images[i], multichannel=True)\n",
    "    # Use a consistent data_range to calculate PSNR\n",
    "    psnr_val = psnr(true_test[i], reconstructed_images[i], data_range=2)\n",
    "\n",
    "    ssim_values.append(ssim_val)\n",
    "    psnr_values.append(psnr_val)\n",
    "\n",
    "# Calculate average SSIM and PSNR values\n",
    "average_ssim = np.mean(ssim_values)\n",
    "average_psnr = np.mean(psnr_values)\n",
    "\n",
    "print(f\"Average SSIM: {average_ssim}\")\n",
    "print(f\"Average PSNR: {average_psnr}\")\n",
    "print(f\"Average R-RMSE: {calculate_rrmse(true_test, reconstructed_images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split encoded features into datasets\n",
    "datasets = np.split(encoded_features, 40)\n",
    "\n",
    "# Split datasets into training and testing indices\n",
    "train_indices, test_indices = train_test_split(np.arange(40), test_size=1/4, shuffle=False)\n",
    "\n",
    "# Function to prepare training and validation data\n",
    "def prepare_data(indices, datasets, input_length=5, forecast_horizon=5):\n",
    "    X, y = [], []\n",
    "    for idx in indices:\n",
    "        data = datasets[idx]\n",
    "\n",
    "        # Generate input-output pairs\n",
    "        for i in range(len(data) - input_length - forecast_horizon + 1):\n",
    "            X.append(data[i:(i + input_length)])\n",
    "            y.append(data[(i + input_length):(i + input_length + forecast_horizon)])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to prepare testing data\n",
    "def prepare_data_image(indices, true_image, input_length=5, forecast_horizon=5):\n",
    "    y = []\n",
    "    for idx in indices:\n",
    "        data = true_image[idx]\n",
    "\n",
    "        # Generate output sequences\n",
    "        for i in range(len(data) - input_length - forecast_horizon + 1):\n",
    "            y.append(data[(i + input_length):(i + input_length + forecast_horizon)])\n",
    "    return np.array(y)\n",
    "\n",
    "# Prepare training and validation data\n",
    "x_train, y_train = prepare_data(train_indices, datasets)\n",
    "x_val, y_val = prepare_data(test_indices, datasets)\n",
    "\n",
    "# Prepare testing data for images\n",
    "y_test_image = prepare_data_image(test_indices, true_image.reshape(40, 300, 64, 64, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalModel(tf.keras.Model):\n",
    "    def __init__(self, n_step=5, feature_dim=128):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.n_step = n_step\n",
    "        self.feature_dim = feature_dim\n",
    "        # Define the model layers\n",
    "        self.lstm1 = tf.keras.layers.LSTM(32, return_sequences=False)\n",
    "        self.repeatVector = tf.keras.layers.RepeatVector(n_step)\n",
    "        self.lstm2 = tf.keras.layers.LSTM(32, return_sequences=True)\n",
    "        self.timeDistributed = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(feature_dim))\n",
    "        self.activation = tf.keras.layers.Activation('relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.repeatVector(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.timeDistributed(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x_true, y_true = data  # Unpack the data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x_true, training=True)  # Forward pass\n",
    "            loss = tf.keras.losses.mean_squared_error(y_true, y_pred)  # Compute loss\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)  # Compute gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))  # Update model parameters\n",
    "\n",
    "        # Update metrics\n",
    "        self.compiled_metrics.update_state(y_true, y_pred)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x_true, y_true = data\n",
    "        y_pred = self(x_true, training=False)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "        # Update metrics\n",
    "        self.compiled_metrics.update_state(y_true, y_pred)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_mean_squared_error',\n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Define model checkpoint callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='./Model_para/CED_LSTM/LSTM-Basic-SW',\n",
    "    monitor='val_mean_squared_error',\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Define reduce learning rate on plateau callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_mean_squared_error',\n",
    "    factor=0.5,\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_lr=1e-4\n",
    ")\n",
    "\n",
    "# Set random seed again for TensorFlow's internal random number generators\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "# Create an instance of OriginalModel\n",
    "Orig_model = OriginalModel(n_step=5, feature_dim=128)\n",
    "\n",
    "# Compile the model\n",
    "Orig_model.compile(optimizer=tf.keras.optimizers.Adam(0.01), metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "Orig_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=150, batch_size=16, callbacks=[model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Reshape the encoded vector to match the input shape expected by the decoder\n",
    "encoded_vector = Orig_model.predict(x_val)[:, 0, :].reshape((-1, 128))  # Assuming encoded_vector is a 1D numpy array\n",
    "\n",
    "# Decode the encoded vector using the decoder model\n",
    "decoded_image = decoder.predict(encoded_vector)\n",
    "\n",
    "# Calculate the total inference time\n",
    "inference_time = time.time() - start_time\n",
    "print(\"Inference time:\", inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_images = y_test_image[:, 0, :, :, :]\n",
    "reconstructed_images = decoded_image\n",
    "\n",
    "def calculate_rrmse(true, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Root Mean Square Error (R-RMSE).\n",
    "    \"\"\"\n",
    "    # Calculate the Root Mean Square Error (RMSE)\n",
    "    rmse = np.sqrt(np.mean((true - predicted) ** 2))\n",
    "\n",
    "    # Calculate the mean of the true data\n",
    "    mean_true = np.mean(true)\n",
    "\n",
    "    # Avoid division by zero by handling the case where the mean of the true data is zero\n",
    "    if mean_true == 0:\n",
    "        return np.inf\n",
    "\n",
    "    # Calculate the Relative Root Mean Square Error (R-RMSE)\n",
    "    rrmse = rmse / mean_true\n",
    "    return rrmse\n",
    "\n",
    "# Initialize SSIM and PSNR lists\n",
    "ssim_values = []\n",
    "psnr_values = []\n",
    "\n",
    "# Calculate SSIM and PSNR for each pair of images\n",
    "for i in range(len(true_images)):\n",
    "    ssim_val = ssim(true_images[i], reconstructed_images[i], multichannel=True)\n",
    "    # Use a consistent data_range to calculate PSNR\n",
    "    psnr_val = psnr(true_images[i], reconstructed_images[i], data_range=2)\n",
    "\n",
    "    ssim_values.append(ssim_val)\n",
    "    psnr_values.append(psnr_val)\n",
    "\n",
    "# Calculate average SSIM and PSNR values\n",
    "average_ssim = np.mean(ssim_values)\n",
    "average_psnr = np.mean(psnr_values)\n",
    "\n",
    "print(f\"Average SSIM: {average_ssim}\")\n",
    "print(f\"Average PSNR: {average_psnr}\")\n",
    "print(f\"Average R-RMSE: {calculate_rrmse(true_images, reconstructed_images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSOVT - CED-LSTM - Physics Contrained Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vor_train = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/vor_train.npy', mmap_mode=\"r\")[:20*300]\n",
    "true_train = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/true_train.npy', mmap_mode=\"r\")[:20*300]\n",
    "vor_test = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/vor_test.npy', mmap_mode=\"r\")\n",
    "true_test = np.load('/content/drive/MyDrive/Physics/Physics/Dataset10/true_test.npy', mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(1)\n",
    "def custom_activation(x):\n",
    "    tanh_activation = tf.keras.activations.tanh(x[:, :, :, :2])\n",
    "    scaled_channel_3 = tf.keras.activations.relu(x[:, :, :, 2])\n",
    "    return tf.concat([tanh_activation, tf.expand_dims(scaled_channel_3, axis=-1)], axis=-1)\n",
    "# Encoder\n",
    "input_img = layers.Input(shape=(64, 64, 3))\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "encoded = layers.Dense(64, activation='relu', name='encoded')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Dense(16*16*64, activation='relu')(encoded)\n",
    "x = layers.Reshape((16, 16, 64))(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2D(3, (3, 3), activation=custom_activation, padding='same', name='decoded')(x)\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Callbacks for adaptive learning rate and early stopping\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    patience=55,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=60,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Training the autoencoder\n",
    "# Make sure autoencoder, vor_train, true_train, vor_test, and true_test are properly defined\n",
    "history = autoencoder.fit(\n",
    "    vor_train,\n",
    "    true_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    validation_data=(vor_test, true_test),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "encoder = tf.keras.models.Model(input_img, encoded)\n",
    "\n",
    "encoded_features = encoder.predict(vor_image)\n",
    "reconstructed_images = autoencoder.predict(vor_image)\n",
    "\n",
    "last_layers = autoencoder.layers[-7:]\n",
    "new_model = tf.keras.models.Sequential(last_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = autoencoder.to_json()\n",
    "with open(\"./Model_para/CED_LSTM/SW-Phy/autoencoder_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "autoencoder.save_weights(\"./Model_para/CED_LSTM/SW-Phy/autoencoder_weights.h5\")\n",
    "print(\"Saved model to disk 😄\")\n",
    "\n",
    "with open(\"./Model_para/CED_LSTM/SW-Phy/model_info.txt\", \"w\") as f:\n",
    "    f.write(\"Training Parameters and Performance Metrics\\n\")\n",
    "    f.write(\"=================================================\\n\")\n",
    "    f.write(\"ReduceLROnPlateau settings:\\n\")\n",
    "    f.write(f\"- Monitor: val_loss\\n\")\n",
    "    f.write(f\"- Factor: 0.2\\n\")\n",
    "    f.write(f\"- Patience: 30\\n\")\n",
    "    f.write(f\"- Min LR: 1e-6\\n\\n\")\n",
    "    f.write(\"EarlyStopping settings:\\n\")\n",
    "    f.write(f\"- Monitor: val_loss\\n\")\n",
    "    f.write(f\"- Patience: 40\\n\")\n",
    "    f.write(f\"- Restore Best Weights: True\\n\\n\")\n",
    "    f.write(\"Training History:\\n\")\n",
    "    for key, values in history.history.items():\n",
    "        f.write(f\"- {key}: {values}\\n\")\n",
    "\n",
    "print(\"All model information has been saved successfully 😄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and testing data\n",
    "vor_image = np.vstack([vor_train, vor_test])\n",
    "true_image = np.vstack([true_train, true_test])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "def custom_activation(x):\n",
    "    \"\"\"\n",
    "    Custom activation function.\n",
    "    Applies tanh activation to the first two channels and ReLU activation to the third channel.\n",
    "    \"\"\"\n",
    "    tanh_activation = tf.keras.activations.tanh(x[:, :, :, :2])\n",
    "    scaled_channel_3 = tf.keras.activations.relu(x[:, :, :, 2])\n",
    "    return tf.concat([tanh_activation, tf.expand_dims(scaled_channel_3, axis=-1)], axis=-1)\n",
    "\n",
    "# Load the model architecture from JSON file\n",
    "with open('./Model_para/CED_LSTM/SW-Phy/autoencoder_architecture.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "autoencoder = model_from_json(loaded_model_json, custom_objects={'custom_activation': custom_activation})\n",
    "\n",
    "# Load weights into the loaded model\n",
    "autoencoder.load_weights(\"./Model_para/CED_LSTM/SW-Phy/autoencoder_weights.h5\")\n",
    "\n",
    "# Extract encoding layers\n",
    "encode_layers = autoencoder.layers[:6]\n",
    "\n",
    "# Create a new Sequential model with the extracted layers\n",
    "encoded = tf.keras.models.Sequential(encode_layers)\n",
    "\n",
    "# Extract last layers\n",
    "last_layers = autoencoder.layers[-7:]\n",
    "\n",
    "# Create a new Sequential model with the extracted last layers\n",
    "new_model = tf.keras.models.Sequential(last_layers)\n",
    "\n",
    "encoded_features = encoded.predict(vor_image)\n",
    "encoded_features_test = encoded.predict(vor_test)\n",
    "reconstructed_images = autoencoder.predict(vor_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, decoder_model, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        # Ensure the decoder_model is a callable (e.g., a Keras model or a function)\n",
    "        if not callable(decoder_model):\n",
    "            raise ValueError(\"decoder_model must be callable\")\n",
    "        self.decoder_model = decoder_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # It's good practice to check if the inputs are valid for the decoder_model\n",
    "        # This can be more specific based on the expected input shape, type, etc.\n",
    "        if inputs is None:\n",
    "            raise ValueError(\"Input to DecoderLayer cannot be None\")\n",
    "\n",
    "        # Use decoder_model to decode the inputs\n",
    "        decoded_images = self.decoder_model(inputs)\n",
    "        return decoded_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = np.split(encoded_features, 30)\n",
    "\n",
    "# 随机选择数据集用作训练和测试\n",
    "train_indices, test_indices = train_test_split(np.arange(30), test_size=1/3, shuffle = False)#random_state=42\n",
    "\n",
    "# 准备训练和测试数据\n",
    "def prepare_data(indices, datasets, input_length=5, forecast_horizon=5):\n",
    "    X, y = [], []\n",
    "    for idx in indices:\n",
    "        data = datasets[idx]\n",
    "\n",
    "        for i in range(len(data) - input_length - forecast_horizon + 1):\n",
    "            X.append(data[i:(i + input_length)])\n",
    "            y.append(data[(i + input_length):(i + input_length + forecast_horizon)])\n",
    "    return np.array(X), np.array(y)\n",
    "x_train, y_train = prepare_data(train_indices, datasets)\n",
    "x_val, y_val = prepare_data(test_indices, datasets)\n",
    "\n",
    "\n",
    "# 准备训练和测试数据\n",
    "def prepare_data(datasets, input_length=5, forecast_horizon=5):\n",
    "    X, y = [], []\n",
    "    data = datasets\n",
    "\n",
    "    for i in range(len(data) - input_length - forecast_horizon + 1):\n",
    "            X.append(data[i:(i + input_length)])\n",
    "            y.append(data[(i + input_length):(i + input_length + forecast_horizon)])\n",
    "    return np.array(X), np.array(y)\n",
    "x_val, y_val = prepare_data(encoded_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalModel(Model):\n",
    "    def __init__(self, n_step=5, feature_dim=256):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.n_step = n_step\n",
    "        self.feature_dim = feature_dim\n",
    "        # Define the model structure\n",
    "        self.lstm1 = LSTM(16, return_sequences=False)\n",
    "        self.repeatVector = RepeatVector(n_step)\n",
    "        self.lstm2 = LSTM(16, return_sequences=True)\n",
    "        self.timeDistributed = TimeDistributed(Dense(feature_dim))\n",
    "        self.activation = Activation('relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.repeatVector(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.timeDistributed(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x_true, y_true = data  # Unpack the data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x_true, training=True)  # Forward pass\n",
    "            loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)  # Compute loss\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)  # Compute gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))  # Update model parameters\n",
    "\n",
    "        energy_error = calculate_relative_energy_error(x_true, y_pred)\n",
    "        mass_loss = calculate_mass_conservation_loss(x_true, y_pred)\n",
    "\n",
    "        # 更新指标\n",
    "        self.compiled_metrics.update_state(y_true, y_pred)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"energy_error\": energy_error, \"mass_loss\": mass_loss})\n",
    "        return results\n",
    "    def test_step(self, data):\n",
    "        x_true, y_true = data\n",
    "        y_pred = self(x_true, training=False)\n",
    "        loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n",
    "        # Calculate custom metrics\n",
    "        energy_error = calculate_relative_energy_error(x_true, y_pred)\n",
    "        mass_loss = calculate_mass_conservation_loss(x_true, y_pred)\n",
    "\n",
    "        # 更新指标\n",
    "        self.compiled_metrics.update_state(y_true, y_pred)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"energy_error\": energy_error, \"mass_loss\": mass_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "tf.random.set_seed(40)\n",
    "\n",
    "Orig_model = OriginalModel(n_step=5, feature_dim=64)\n",
    "\n",
    "Orig_model.compile(optimizer=tf.keras.optimizers.Adam(0.01), metrics=['mean_squared_error'])#, metrics=['mean_squared_error',calculate_relative_energy_error,calculate_mass_conservation_loss]\n",
    "\n",
    "Orig_model.fit(x_train, y_train,validation_data = (x_val,y_val),epochs=100, batch_size=64,callbacks = [model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Dataset_sw/x_train_LSTM.npy', x_train)\n",
    "np.save('./Dataset_sw/y_train_LSTM.npy', y_train)\n",
    "np.save('./Dataset_sw/x_val_LSTM.npy', x_val)\n",
    "np.save('./Dataset_sw/y_val_LSTM.npy', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./Dataset_sw/x_train_LSTM.npy',mmap_mode = 'r')\n",
    "y_train = np.load('./Dataset_sw/y_train_LSTM.npy',mmap_mode = 'r')\n",
    "x_val = np.load('./Dataset_sw/x_val_LSTM.npy',mmap_mode = 'r')\n",
    "y_val = np.load('./Dataset_sw/y_val_LSTM.npy',mmap_mode = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = DecoderLayer(new_model)\n",
    "\n",
    "x=64\n",
    "n=5\n",
    "def preprocess_images(y_true, y_pred):\n",
    "    decoder_layer = DecoderLayer(new_model)\n",
    "    y_pred = tf.reshape(y_pred, (-1, x))\n",
    "    y_true = tf.reshape(y_true, (-1, x))\n",
    "    y_pred = decoder_layer(y_pred)\n",
    "    y_true = decoder_layer(y_true)\n",
    "    y_pred = tf.reshape(y_pred, (-1, n, 64, 64, 3))\n",
    "    y_true = tf.reshape(y_true, (-1, n, 64, 64, 3))\n",
    "    return y_true, y_pred\n",
    "\n",
    "def calculate_image_energy(img):\n",
    "    g = 9.81\n",
    "    dx = 1\n",
    "    img = tf.reshape(img, (-1, img.shape[2], img.shape[3],3))\n",
    "\n",
    "    kinetic_energy = 0.5 * tf.reduce_sum(tf.square(img[..., 0]), axis=(1, 2)) * dx**2\n",
    "    kinetic_energy += 0.5 * tf.reduce_sum(tf.square(img[..., 1]), axis=(1, 2)) * dx**2\n",
    "    # Calculate potential energy\n",
    "    potential_energy = 0.5 * g * tf.reduce_sum(tf.square(img[..., 2]), axis=(1, 2)) * dx**2\n",
    "    # Calculate total energy\n",
    "    total_energy = kinetic_energy + potential_energy\n",
    "    return total_energy\n",
    "\n",
    "\n",
    "def calculate_relative_energy_error(y_true, y_pred):\n",
    "    y_true, y_pred = preprocess_images(y_true, y_pred)\n",
    "    energy_true = calculate_image_energy(y_true)\n",
    "    energy_pred = calculate_image_energy(y_pred)\n",
    "    return tf.reduce_sum(tf.abs(energy_pred - energy_true))\n",
    "'''\n",
    "Example Physics Constraints\n",
    "def calculate_mass_conservation(y):\n",
    "    y = tf.reshape(y, (-1, y.shape[2], y.shape[3], 3))\n",
    "    h, u, v = y[..., 2:3], y[..., 0:1], y[..., 1:2]\n",
    "    dh_dx, dh_dy = tf.image.image_gradients(h)\n",
    "    du_dx, _ = tf.image.image_gradients(u)\n",
    "    _, dv_dy = tf.image.image_gradients(v)\n",
    "    mass_conservation = dh_dx * u + h * du_dx + dh_dy * v + h * dv_dy\n",
    "\n",
    "    mass_conservation_loss = tf.reduce_sum(tf.abs(mass_conservation), axis=[1, 2, 3])\n",
    "    return mass_conservation_loss\n",
    "\n",
    "def calculate_mass_conservation_loss(y_true, y_pred):\n",
    "    y_true, y_pred = preprocess_images(y_true, y_pred)\n",
    "    mass_conservation_true = calculate_mass_conservation(y_true)\n",
    "    mass_conservation_pred = calculate_mass_conservation(y_pred)\n",
    "    return tf.reduce_mean(tf.abs(mass_conservation_pred - mass_conservation_true))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='./Model_para/CED_LSTM/LSTM-Physics-SW',  # Note the removed file extension\n",
    "    monitor='val_mean_squared_error',  # It's good practice to monitor 'val_loss' for consistency\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    save_format='tf'  # Specify save_format as 'tf'\n",
    ")\n",
    "\n",
    "\n",
    "alpha = tf.Variable(5e-10, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def custom_loss(x_true, y_true, y_pred):\n",
    "    mse_loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    energy_restriction = calculate_relative_energy_error(x_true, y_pred)\n",
    "    return mse_loss + alpha* energy_restriction\n",
    "\n",
    "class PhysicsInformedLSTM(Model):\n",
    "    def __init__(self, n_step=10, feature_dim=128):\n",
    "        super(PhysicsInformedLSTM, self).__init__()\n",
    "        self.n_step = n_step\n",
    "        self.feature_dim = feature_dim\n",
    "        self.lstm1 = LSTM(16, return_sequences=False)\n",
    "        self.repeatVector = RepeatVector(n_step)\n",
    "        self.lstm2 = LSTM(16, return_sequences=True)\n",
    "        self.timeDistributed = TimeDistributed(Dense(feature_dim))\n",
    "        self.activation = Activation('relu')\n",
    "        # Define custom metrics\n",
    "        self.energy_error_metric = tf.keras.metrics.Mean(name='energy_error')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.repeatVector(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.timeDistributed(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x_true, y_true = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x_true, training=True)\n",
    "            loss = custom_loss(x_true, y_true, y_pred)\n",
    "            # Calculate custom metrics\n",
    "            energy_error = calculate_relative_energy_error(x_true, y_pred)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Update built-in and custom metrics\n",
    "        self.compiled_metrics.update_state(y_true, y_pred)\n",
    "        self.energy_error_metric.update_state(energy_error)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        # Include custom metrics in the returned dictionary\n",
    "        results.update({'energy_error': self.energy_error_metric.result()})\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x_true, y_true = data\n",
    "        y_pred = self(x_true, training=False)\n",
    "        loss = custom_loss(x_true, y_true, y_pred)\n",
    "        # Calculate custom metrics\n",
    "        energy_error = calculate_relative_energy_error(x_true, y_pred)\n",
    "        # Update built-in and custom metrics\n",
    "        self.compiled_metrics.update_state(y_true, y_pred)\n",
    "        self.energy_error_metric.update_state(energy_error)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        # Include custom metrics in the returned dictionary\n",
    "        results.update({'energy_error': self.energy_error_metric.result()})\n",
    "        return results\n",
    "\n",
    "    # Reset custom metrics after each epoch\n",
    "    def reset_metrics(self):\n",
    "        super(PhysicsInformedLSTM, self).reset_metrics()\n",
    "        self.energy_error_metric.reset_states()\n",
    "# Instantiate the PhysicsInformedLSTM model\n",
    "model = PhysicsInformedLSTM(n_step=5, feature_dim=64)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), metrics=['mean_squared_error'])\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Custom callback to adjust alpha\n",
    "class AdjustAlphaCallback(Callback):\n",
    "    def __init__(self, alpha_var, new_value, change_epoch):\n",
    "        super().__init__()\n",
    "        self.alpha_var = alpha_var\n",
    "        self.new_value = new_value\n",
    "        self.change_epoch = change_epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch == self.change_epoch:\n",
    "            tf.keras.backend.set_value(self.alpha_var, self.new_value)\n",
    "            print(f\"Alpha has been updated to {self.new_value} at epoch {epoch+1}\")\n",
    "\n",
    "# Instantiate the callback with the desired settings\n",
    "adjust_alpha_callback = AdjustAlphaCallback(alpha, new_value=1e-9, change_epoch=50)  # Epochs are 0-indexed, so epoch 29 is the 30th epoch\n",
    "\n",
    "# Now include this callback in your model's fit method along with other callbacks\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=64, callbacks=[model_checkpoint, adjust_alpha_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='mean_squared_error',\n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Define the model checkpoint callback\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='./Model_para/CED_LSTM/LSTM-BP-SW',  # Path where to save the model\n",
    "    monitor='val_mean_squared_error',\n",
    "    save_best_only=True, \n",
    "    verbose=1,\n",
    "    mode='min' \n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='mean_squared_error',\n",
    "    factor=0.5, \n",
    "    patience=10, \n",
    "    verbose=1,\n",
    "    mode='min',  \n",
    "    min_lr=1e-6  \n",
    ")\n",
    "\n",
    "np.random.seed(40)\n",
    "tf.random.set_seed(40)\n",
    "\n",
    "\n",
    "Orig_model = OriginalModel(n_step=5, feature_dim=64)\n",
    "\n",
    "Orig_model.compile(optimizer=tf.keras.optimizers.Adam(0.01), metrics=['mean_squared_error'])#, metrics=['mean_squared_error',calculate_relative_energy_error,calculate_mass_conservation_loss]\n",
    "\n",
    "Orig_model.fit(x_train, y_train,validation_data = (x_val,y_val),epochs=100, batch_size=64,callbacks = [model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_path = './Model_para/CED_LSTM/LSTM-BP-SW'\n",
    "physical_model_path = './Model_para/CED_LSTM/LSTM-Physics-SW'\n",
    "\n",
    "original_model = tf.keras.models.load_model(original_model_path, custom_objects={\n",
    "    'calculate_relative_energy_error': calculate_relative_energy_error,\n",
    "    'calculate_mass_conservation_loss': calculate_mass_conservation_loss\n",
    "})\n",
    "\n",
    "physical_model = tf.keras.models.load_model(physical_model_path, custom_objects={\n",
    "    'custom_loss': custom_loss,\n",
    "    'calculate_relative_energy_error': calculate_relative_energy_error,\n",
    "    'calculate_mass_conservation_loss': calculate_mass_conservation_loss\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "x_train_cnn, x_test_cnn = train_test_split(encoded_features, test_size=1/3, shuffle=False)\n",
    "reshaped_features = x_test_cnn.reshape((10, 300, 64))\n",
    "\n",
    "# Extract features for original model\n",
    "extracted_features_batches = []\n",
    "x = 5\n",
    "for feature_batch in reshaped_features:\n",
    "    extracted_batch = np.array([feature_batch[i:i+x] for i in range(0, feature_batch.shape[0] - 10, x)])\n",
    "    extracted_features_batches.append(extracted_batch)\n",
    "extracted_features_batches = np.array(extracted_features_batches)\n",
    "\n",
    "# Extract true images for physics-based model\n",
    "reshaped_features = true_test.reshape((10, 300, 64, 64, 3))\n",
    "extracted_true_image = []\n",
    "x = 5\n",
    "for feature_batch in reshaped_features:\n",
    "    extracted_batch = np.array([feature_batch[i:i+x] for i in range(10, feature_batch.shape[0], x)])\n",
    "    extracted_true_image.append(extracted_batch)\n",
    "extracted_true_image = np.array(extracted_true_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_POSITION = 15\n",
    "STEP_SIZE = 5\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "def rolling_forecast(model, initial_input, true_images):\n",
    "    input_sequence = initial_input.reshape(1, STEP_SIZE, SEQUENCE_LENGTH)  # Reshape to match LSTM input shape\n",
    "    predictions = []\n",
    "    mse_array = []\n",
    "    true_images_array = []\n",
    "    for i in range(42):  # Consider 5 steps at a time\n",
    "\n",
    "        next_steps = model.predict(input_sequence)\n",
    "        next_steps_2d = next_steps.reshape(-1, SEQUENCE_LENGTH)\n",
    "\n",
    "        predicted_image = decoder_layer(next_steps_2d)\n",
    "        true_image = true_images[START_POSITION + i]\n",
    "\n",
    "        predictions.extend(next_steps_2d)\n",
    "        mse = calculate_rrmse(true_image,predicted_image).reshape(-1, 1)#np.mean((predicted_image - true_image) ** 2, axis=(1, 2, 3)).reshape(-1, 1)\n",
    "\n",
    "        mse_array.append(mse)\n",
    "        true_images_array.append(true_image)\n",
    "        input_sequence = next_steps\n",
    "    return np.array(predictions), np.array(mse_array), np.array(true_images_array)\n",
    "\n",
    "def prepare_data(features_batches, true_image_batches, batch_index):\n",
    "    return features_batches[batch_index], true_image_batches[batch_index]\n",
    "\n",
    "n = 1\n",
    "features_batches, true_image_batches = extracted_features_batches[n:(n+9)], extracted_true_image[n:(n+9)]\n",
    "\n",
    "all_mse_orig = []\n",
    "all_mse_phy = []\n",
    "pred_orig_list = []\n",
    "pred_phy_list = []\n",
    "\n",
    "true_orig_list = []  # Renamed from t_orig_list\n",
    "true_phy_list = []  # Renamed from t_phy_list\n",
    "\n",
    "hist_mse_orig = []\n",
    "hist_mse_phy = []\n",
    "# Collect MSE for each data set\n",
    "for i in range(9):\n",
    "    x_test_cu, y_test_cu = prepare_data(features_batches, true_image_batches, i)\n",
    "    initial_input = x_test_cu[START_POSITION, :, :]\n",
    "\n",
    "    pred_orig, mse_orig, true_orig = rolling_forecast(original_model, initial_input, y_test_cu)\n",
    "    pred_phy, mse_phy, true_phy = rolling_forecast(physical_model, initial_input, y_test_cu)\n",
    "\n",
    "    avg_mse_per_group_orig = np.mean(mse_orig, axis=1)\n",
    "    avg_mse_per_group_phy = np.mean(mse_phy, axis=1)\n",
    "    hist_mse_orig.append(mse_orig)\n",
    "    hist_mse_phy.append(mse_phy)\n",
    "\n",
    "    all_mse_orig.append(avg_mse_per_group_orig)\n",
    "    all_mse_phy.append(avg_mse_per_group_phy)\n",
    "    pred_orig = decoder_layer(pred_orig)\n",
    "    pred_phy = decoder_layer(pred_phy)\n",
    "    pred_orig_list.append(pred_orig)\n",
    "    pred_phy_list.append(pred_phy)\n",
    "    true_orig_list.append(true_orig)\n",
    "    true_phy_list.append(true_phy)\n",
    "\n",
    "all_mse_orig = np.array(all_mse_orig)\n",
    "all_mse_phy = np.array(all_mse_phy)\n",
    "\n",
    "# Filter out groups with maximum MSE below the threshold\n",
    "threshold = 1\n",
    "is_below_threshold_orig = np.max(all_mse_orig, axis=(1, 2)) <= threshold\n",
    "filtered_mse_orig = all_mse_orig[is_below_threshold_orig]\n",
    "\n",
    "is_below_threshold_phy = np.max(all_mse_phy, axis=(1, 2)) <= threshold\n",
    "filtered_mse_phy = all_mse_phy[is_below_threshold_phy]\n",
    "\n",
    "# Calculate average and standard deviation of filtered MSE\n",
    "avg_mse_orig = np.mean(filtered_mse_orig, axis=0)\n",
    "std_mse_orig = np.std(filtered_mse_orig, axis=0)\n",
    "\n",
    "avg_mse_phy = np.mean(filtered_mse_phy, axis=0)\n",
    "std_mse_phy = np.std(filtered_mse_phy, axis=0)\n",
    "\n",
    "# Flatten arrays for plotting\n",
    "avg_mse_orig = np.asarray(avg_mse_orig).flatten()\n",
    "std_mse_orig = np.asarray(std_mse_orig).flatten()\n",
    "avg_mse_phy = np.asarray(avg_mse_phy).flatten()\n",
    "std_mse_phy = np.asarray(std_mse_phy).flatten()\n",
    "\n",
    "# Plot aggregated MSE with standard deviation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(avg_mse_orig, label='Original Model Average MSE')\n",
    "plt.fill_between(range(len(avg_mse_orig)),\n",
    "                 avg_mse_orig - std_mse_orig,\n",
    "                 avg_mse_orig + std_mse_orig,\n",
    "                 alpha=0.2)\n",
    "plt.plot(avg_mse_phy, label='Physics-based Model Average MSE')\n",
    "plt.fill_between(range(len(avg_mse_phy)), avg_mse_phy - std_mse_phy, avg_mse_phy + std_mse_phy, alpha=0.2)\n",
    "\n",
    "plt.title('Aggregated Images R-RMSE with Standard Deviation over Steps Compared to Decoded True Fields')\n",
    "plt.xlabel('Step', fontsize=16)\n",
    "plt.ylabel('MSE', fontsize=16)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mse_orig = np.array(hist_mse_orig).reshape((-1,1))\n",
    "hist_mse_phy = np.array(hist_mse_phy).reshape((-1,1))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "color_orig_hist = 'skyblue'\n",
    "color_phy_hist = 'orange'\n",
    "edge_color = 'black'\n",
    "plt.hist(hist_mse_orig, bins=30, alpha=0.75, color='skyblue', edgecolor='black', label='Basic CED-LSTM Model')\n",
    "plt.hist(hist_mse_phy, bins=30, alpha=0.75, color='orange', edgecolor='black', label='Physics-based CED-LSTM Model')\n",
    "\n",
    "plt.xlabel('Error Value', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('Histogram of MSE Values for Two Models', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_POSITION = 15\n",
    "STEP_SIZE = 5\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "def rolling_forecast(model, initial_input, true_images):\n",
    "    input_sequence = initial_input.reshape(1, STEP_SIZE, SEQUENCE_LENGTH)  # Reshape to match LSTM input shape\n",
    "    predictions = []\n",
    "    ssim_array = []\n",
    "    true_images_array = []\n",
    "    for i in range(42):  # Consider 5 steps at a time\n",
    "        next_steps = model.predict(input_sequence)\n",
    "        next_steps_2d = next_steps.reshape(-1, SEQUENCE_LENGTH)\n",
    "\n",
    "        predicted_image = decoder_layer(next_steps_2d)\n",
    "        true_image = true_images[START_POSITION + i]\n",
    "\n",
    "        # Convert TensorFlow tensor to NumPy array\n",
    "        predicted_image_np = predicted_image.numpy()\n",
    "\n",
    "        predictions.extend(next_steps_2d)\n",
    "        ssim_val, _ = ssim(predicted_image_np, true_image, multichannel=True, full=True, win_size=5)\n",
    "        ssim_array.append([ssim_val])\n",
    "        true_images_array.append(true_image)\n",
    "        input_sequence = next_steps\n",
    "    return np.array(predictions), np.array(ssim_array), np.array(true_images_array)\n",
    "\n",
    "def prepare_data(features_batches, true_image_batches, batch_index):\n",
    "    return features_batches[batch_index], true_image_batches[batch_index]\n",
    "\n",
    "n = 1\n",
    "features_batches, true_image_batches = extracted_features_batches[n:(n+9)], extracted_true_image[n:(n+9)]\n",
    "\n",
    "all_ssim_orig = []\n",
    "all_ssim_phy = []\n",
    "pred_orig_list = []\n",
    "pred_phy_list = []\n",
    "\n",
    "true_orig_list = []  # Renamed from t_orig_list\n",
    "true_phy_list = []  # Renamed from t_phy_list\n",
    "\n",
    "# Collect SSIM for each data set\n",
    "for i in range(9):\n",
    "    x_test_cu, y_test_cu = prepare_data(features_batches, true_image_batches, i)\n",
    "    initial_input = x_test_cu[START_POSITION, :, :]\n",
    "\n",
    "    pred_orig, ssim_orig, true_orig = rolling_forecast(original_model, initial_input, y_test_cu)\n",
    "    pred_phy, ssim_phy, true_phy = rolling_forecast(physical_model, initial_input, y_test_cu)\n",
    "\n",
    "    avg_ssim_per_group_orig = np.mean(ssim_orig, axis=1)\n",
    "    avg_ssim_per_group_phy = np.mean(ssim_phy, axis=1)\n",
    "\n",
    "    all_ssim_orig.append(avg_ssim_per_group_orig)\n",
    "    all_ssim_phy.append(avg_ssim_per_group_phy)\n",
    "    pred_orig = decoder_layer(pred_orig)\n",
    "    pred_phy = decoder_layer(pred_phy)\n",
    "    pred_orig_list.append(pred_orig)\n",
    "    pred_phy_list.append(pred_phy)\n",
    "    true_orig_list.append(true_orig)\n",
    "    true_phy_list.append(true_phy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_POSITION = 15\n",
    "STEP_SIZE = 5\n",
    "SEQUENCE_LENGTH = 64\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "def rolling_forecast(model, initial_input, true_images):\n",
    "    input_sequence = initial_input.reshape(1, STEP_SIZE, SEQUENCE_LENGTH)  # Reshape to match LSTM input shape\n",
    "    predictions = []\n",
    "    psnr_array = []\n",
    "    true_images_array = []\n",
    "    for i in range(42):  # Consider 5 steps at a time\n",
    "        next_steps = model.predict(input_sequence)\n",
    "        next_steps_2d = next_steps.reshape(-1, SEQUENCE_LENGTH)\n",
    "\n",
    "        predicted_image = decoder_layer(next_steps_2d)\n",
    "        true_image = true_images[START_POSITION + i]\n",
    "\n",
    "        # Convert TensorFlow tensor to NumPy array\n",
    "        predicted_image_np = predicted_image.numpy()\n",
    "\n",
    "        predictions.extend(next_steps_2d)\n",
    "        psnr_val = psnr(true_image, predicted_image_np, data_range=predicted_image_np.max() - predicted_image_np.min())\n",
    "        psnr_array.append([psnr_val])\n",
    "        true_images_array.append(true_image)\n",
    "        input_sequence = next_steps\n",
    "    return np.array(predictions), np.array(psnr_array), np.array(true_images_array)\n",
    "\n",
    "\n",
    "def prepare_data(features_batches, true_image_batches, batch_index):\n",
    "    return features_batches[batch_index], true_image_batches[batch_index]\n",
    "\n",
    "n = 1\n",
    "features_batches, true_image_batches = extracted_features_batches[n:(n+9)], extracted_true_image[n:(n+9)]\n",
    "\n",
    "all_psnr_orig = []\n",
    "all_psnr_phy = []\n",
    "pred_orig_list = []\n",
    "pred_phy_list = []\n",
    "\n",
    "true_orig_list = []  # Renamed from t_orig_list\n",
    "true_phy_list = []  # Renamed from t_phy_list\n",
    "\n",
    "# Collect PSNR for each data set\n",
    "for i in range(9):\n",
    "    x_test_cu, y_test_cu = prepare_data(features_batches, true_image_batches, i)\n",
    "    initial_input = x_test_cu[START_POSITION, :, :]\n",
    "\n",
    "    pred_orig, psnr_orig, true_orig = rolling_forecast(original_model, initial_input, y_test_cu)\n",
    "    pred_phy, psnr_phy, true_phy = rolling_forecast(physical_model, initial_input, y_test_cu)\n",
    "\n",
    "    avg_psnr_per_group_orig = np.mean(psnr_orig, axis=1)\n",
    "    avg_psnr_per_group_phy = np.mean(psnr_phy, axis=1)\n",
    "\n",
    "    all_psnr_orig.append(avg_psnr_per_group_orig)\n",
    "    all_psnr_phy.append(avg_psnr_per_group_phy)\n",
    "    pred_orig = decoder_layer(pred_orig)\n",
    "    pred_phy = decoder_layer(pred_phy)\n",
    "    pred_orig_list.append(pred_orig)\n",
    "    pred_phy_list.append(pred_phy)\n",
    "    true_orig_list.append(true_orig)\n",
    "    true_phy_list.append(true_phy)\n",
    "\n",
    "all_psnr_orig = np.array(all_psnr_orig)\n",
    "all_psnr_phy = np.array(all_psnr_phy)\n",
    "\n",
    "filtered_psnr_orig = all_psnr_orig\n",
    "filtered_psnr_phy = all_psnr_phy\n",
    "\n",
    "# Calculate average and standard deviation of filtered PSNR\n",
    "avg_psnr_orig = np.mean(filtered_psnr_orig, axis=0)\n",
    "std_psnr_orig = np.std(filtered_psnr_orig, axis=0)\n",
    "\n",
    "avg_psnr_phy = np.mean(filtered_psnr_phy, axis=0)\n",
    "std_psnr_phy = np.std(filtered_psnr_phy, axis=0)\n",
    "\n",
    "# Flatten arrays for plotting\n",
    "avg_psnr_orig = np.asarray(avg_psnr_orig).flatten()\n",
    "std_psnr_orig = np.asarray(std_psnr_orig).flatten()\n",
    "avg_psnr_phy = np.asarray(avg_psnr_phy).flatten()\n",
    "std_psnr_phy = np.asarray(std_psnr_phy).flatten()\n",
    "\n",
    "# Plot aggregated PSNR with standard deviation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(avg_psnr_orig, label='Original Model Average PSNR')\n",
    "plt.fill_between(range(len(avg_psnr_orig)),\n",
    "                 avg_psnr_orig - std_psnr_orig,\n",
    "                 avg_psnr_orig + std_psnr_orig,\n",
    "                 alpha=0.2)\n",
    "plt.plot(avg_psnr_phy, label='Physics-based Model Average PSNR')\n",
    "plt.fill_between(range(len(avg_psnr_phy)), avg_psnr_phy - std_psnr_phy, avg_psnr_phy + std_psnr_phy, alpha=0.2)\n",
    "\n",
    "plt.title('Aggregated Images PSNR with Standard Deviation over Steps Compared to True Fields')\n",
    "plt.xlabel('Step', fontsize=16)\n",
    "plt.ylabel('PSNR', fontsize=16)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Define color palette\n",
    "color_orig = 'blue'\n",
    "color_phy = 'red'\n",
    "color_fill_orig = 'lightblue'\n",
    "color_fill_phy = 'salmon'  # Using 'salmon' as a lighter shade of red\n",
    "\n",
    "# Plot MSE\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(avg_mse_orig, label='Basic Model', color=color_orig)\n",
    "plt.fill_between(range(len(avg_mse_orig)),\n",
    "                 avg_mse_orig - std_mse_orig,\n",
    "                 avg_mse_orig + std_mse_orig,\n",
    "                 alpha=0.2, color=color_fill_orig)\n",
    "plt.plot(avg_mse_phy, label='Physics-based Model', color=color_phy)\n",
    "plt.fill_between(range(len(avg_mse_phy)),\n",
    "                 avg_mse_phy - std_mse_phy,\n",
    "                 avg_mse_phy + std_mse_phy,\n",
    "                 alpha=0.2, color=color_fill_phy)\n",
    "plt.title('R-RMSE with Standard Deviation over Steps', fontsize=16)\n",
    "plt.xlabel('Step', fontsize=12)\n",
    "plt.ylabel('R-RMSE', fontsize=12)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot SSIM\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(avg_ssim_orig, label='Basic Model', color=color_orig)\n",
    "plt.fill_between(range(len(avg_ssim_orig)),\n",
    "                 avg_ssim_orig - std_ssim_orig,\n",
    "                 avg_ssim_orig + std_ssim_orig,\n",
    "                 alpha=0.2, color=color_fill_orig)\n",
    "plt.plot(avg_ssim_phy, label='Physics-based Model', color=color_phy)\n",
    "plt.fill_between(range(len(avg_ssim_phy)),\n",
    "                 avg_ssim_phy - std_ssim_phy,\n",
    "                 avg_ssim_phy + std_ssim_phy,\n",
    "                 alpha=0.2, color=color_fill_phy)\n",
    "plt.title('SSIM with Standard Deviation over Steps', fontsize=16)\n",
    "plt.xlabel('Step', fontsize=12)\n",
    "plt.ylabel('SSIM', fontsize=12)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot PSNR\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(avg_psnr_orig, label='Basic Model', color=color_orig)\n",
    "plt.fill_between(range(len(avg_psnr_orig)),\n",
    "                 avg_psnr_orig - std_psnr_orig,\n",
    "                 avg_psnr_orig + std_psnr_orig,\n",
    "                 alpha=0.2, color=color_fill_orig)\n",
    "plt.plot(avg_psnr_phy, label='Physics-based Model', color=color_phy)\n",
    "plt.fill_between(range(len(avg_psnr_phy)),\n",
    "                 avg_psnr_phy - std_psnr_phy,\n",
    "                 avg_psnr_phy + std_psnr_phy,\n",
    "                 alpha=0.2, color=color_fill_phy)\n",
    "plt.title('PSNR with Standard Deviation over Steps', fontsize=16)\n",
    "plt.xlabel('Step', fontsize=12)\n",
    "plt.ylabel('PSNR', fontsize=12)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
