{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/DSVOT/blob/main/NOAA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwm99IJ3icJG"
      },
      "outputs": [],
      "source": [
        "pip install pykrige"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo5kgwo7uF9A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from scipy.interpolate import griddata\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D, Cropping2D,\n",
        "    AveragePooling2D, Flatten, Reshape, Dropout, Conv2D, LSTM,\n",
        "    RepeatVector\n",
        ")\n",
        "from tensorflow.keras.losses import MSE\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from pykrige.ok3d import OrdinaryKriging3D\n",
        "from pykrige.uk3d import UniversalKriging3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiDC3zr63Qvo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKjXwDd8uF9D"
      },
      "source": [
        "# 真实无预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKc9tOtp2Mtl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "def generate_data(f, sen_num_kind_list, sen_num_var_list):\n",
        "    lat = np.array(f['lat'])\n",
        "    lon = np.array(f['lon'])\n",
        "    sst = np.array(f['sst'])\n",
        "    sst1 = np.nan_to_num(sst)\n",
        "    sst_reshape = sst[0, :].reshape(len(lat[0, :]), len(lon[0, :]), order='F')\n",
        "    xv1, yv1 = np.meshgrid(lon[0, :], lat[0, :])\n",
        "\n",
        "    X_ki = np.zeros((1040 * len(sen_num_kind_list) * len(sen_num_var_list), len(lat[0, :]), len(lon[0, :]), 2))\n",
        "    y_ki = np.zeros((1040 * len(sen_num_kind_list) * len(sen_num_var_list), len(lat[0, :]), len(lon[0, :]), 1))\n",
        "\n",
        "    for ki in range(len(sen_num_kind_list)):\n",
        "        sen_num = sen_num_kind_list[ki]\n",
        "        X_va = np.zeros((1040 * len(sen_num_var_list), len(lat[0, :]), len(lon[0, :]), 2))\n",
        "        y_va = np.zeros((1040 * len(sen_num_var_list), len(lat[0, :]), len(lon[0, :]), 1))\n",
        "\n",
        "        for va in range(len(sen_num_var_list)):\n",
        "            X_t = np.zeros((1040, len(lat[0, :]), len(lon[0, :]), 2))\n",
        "            y_t = np.zeros((1040, len(lat[0, :]), len(lon[0, :]), 1))\n",
        "\n",
        "            for t in range(1040):\n",
        "                y_t[t, :, :, 0] = np.nan_to_num(sst[t, :].reshape(len(lat[0, :]), len(lon[0, :]), order='F'))\n",
        "\n",
        "                np.random.seed(sen_num_var_list[va])\n",
        "                sparse_locations_lat = np.random.randint(len(lat[0, :]), size=(sen_num))\n",
        "                sparse_locations_lon = np.random.randint(len(lon[0, :]), size=(sen_num))\n",
        "\n",
        "                sparse_locations = np.column_stack((sparse_locations_lat, sparse_locations_lon))\n",
        "\n",
        "                for s in range(sen_num):\n",
        "                  a, b = sparse_locations[s]\n",
        "                  while np.isnan(sst_reshape[int(a), int(b)]) == True:\n",
        "                    a = np.random.randint(len(lat[0, :]), size=(1))\n",
        "                    b = np.random.randint(len(lon[0, :]), size=(1))\n",
        "                    sparse_locations[s, 0] = a\n",
        "                    sparse_locations[s, 1] = b\n",
        "\n",
        "\n",
        "                sparse_data = np.zeros((sen_num))\n",
        "                for s in range(sen_num):\n",
        "                    sparse_data[s] = (y_t[t, :, :, 0][int(sparse_locations[s, 0]), int(sparse_locations[s, 1])])\n",
        "\n",
        "                sparse_locations_ex = np.zeros(sparse_locations.shape)\n",
        "                for i in range(sen_num):\n",
        "                    sparse_locations_ex[i, 0] = lat[0, :][int(sparse_locations[i, 0])]\n",
        "                    sparse_locations_ex[i, 1] = lon[0, :][int(sparse_locations[i, 1])]\n",
        "\n",
        "                grid_z0 = griddata(sparse_locations_ex, sparse_data, (yv1, xv1), method='nearest')\n",
        "                for j in range(len(lon[0, :])):\n",
        "                    for i in range(len(lat[0, :])):\n",
        "                        if np.isnan(sst_reshape[i, j]):\n",
        "                            grid_z0[i, j] = 0\n",
        "                X_t[t, :, :, 0] = grid_z0\n",
        "                mask_img = np.zeros(grid_z0.shape)\n",
        "                for i in range(sen_num):\n",
        "                    mask_img[int(sparse_locations[i, 0]), int(sparse_locations[i, 1])] = 1\n",
        "                X_t[t, :, :, 1] = mask_img\n",
        "\n",
        "            X_va[1040 * va:1040 * (va + 1), :, :, :] = X_t\n",
        "            y_va[1040 * va:1040 * (va + 1), :, :, :] = y_t\n",
        "\n",
        "        X_ki[(1040 * len(sen_num_var_list)) * ki:(1040 * len(sen_num_var_list)) * (ki + 1), :, :, :] = X_va\n",
        "        y_ki[(1040 * len(sen_num_var_list)) * ki:(1040 * len(sen_num_var_list)) * (ki + 1), :, :, :] = y_va\n",
        "\n",
        "    return X_ki, y_ki\n",
        "\n",
        "\n",
        "f = h5py.File('/content/drive/MyDrive/Physics/Physics/sst_weekly.mat', 'r')\n",
        "sen_num_kind_list = [200,240,280,320]\n",
        "sen_num_var_list = [300, 100,10]\n",
        "\n",
        "X_ki, y_ki = generate_data(f, sen_num_kind_list, sen_num_var_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT7ow6rVWbvb"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_train.npy',X_ki)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_train.npy',y_ki)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts6lS4ynaWPN"
      },
      "outputs": [],
      "source": [
        "sen_num_kind_list = [200,240,280,300,320,340]\n",
        "sen_num_var_list = [900]\n",
        "\n",
        "X_ki, y_ki = generate_data(f, sen_num_kind_list, sen_num_var_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n17rwt44aiRx"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_test.npy',X_ki)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_test.npy',y_ki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMym2LN1LyQw"
      },
      "source": [
        "# VOR-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmVvv-1FQ-G8"
      },
      "outputs": [],
      "source": [
        "x_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_train.npy',mmap_mode = 'r')\n",
        "y_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_train.npy',mmap_mode = 'r')\n",
        "x_test = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_test.npy',mmap_mode = 'r')\n",
        "y_test = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_test.npy',mmap_mode = 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNtpTH_IM97t"
      },
      "outputs": [],
      "source": [
        "f = h5py.File('/content/drive/MyDrive/Physics/Physics/sst_weekly.mat', 'r')\n",
        "lat = np.array(f['lat'])\n",
        "lon = np.array(f['lon'])\n",
        "sst = np.array(f['sst'])\n",
        "time = np.array(f['time'])\n",
        "\n",
        "sst1= np.nan_to_num(sst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjRcc7GlLxu_"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.layers import Input, Add, Dense, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Flatten, Reshape, LSTM\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from scipy.spatial import Voronoi\n",
        "import math\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "input_img = Input(shape=(len(lat[0,:]),len(lon[0,:]),2))\n",
        "x = Conv2D(48, (7,7),activation='relu', padding='same')(input_img)\n",
        "x = Conv2D(48, (7,7),activation='relu', padding='same')(x)\n",
        "x = Conv2D(48, (7,7),activation='relu', padding='same')(x)\n",
        "x = Conv2D(48, (7,7),activation='relu', padding='same')(x)\n",
        "x = Conv2D(48, (7,7),activation='relu', padding='same')(x)\n",
        "x = Conv2D(48, (7,7),activation='relu', padding='same')(x)\n",
        "x = Conv2D(48, (7,7),activation='relu', padding='same')(x)\n",
        "x_final = Conv2D(1, (7,7), padding='same')(x)\n",
        "model = Model(input_img, x_final)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "model_cb=ModelCheckpoint('/content/drive/MyDrive/Physics/NOAA/Model_NOAA.h5', monitor='val_loss',save_best_only=True,verbose=1)\n",
        "early_cb=EarlyStopping(monitor='val_loss', patience=50,verbose=1)\n",
        "cb = [model_cb, early_cb]\n",
        "history = model.fit(x_train,y_train,epochs=150,batch_size=32,verbose=1,callbacks=cb,shuffle=True,validation_data=[x_test, y_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V4BD28zLzwl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model_cb=ModelCheckpoint('/content/drive/MyDrive/Physics/NOAA/Model_NOAA.h5', monitor='val_loss',save_best_only=True,verbose=1)\n",
        "early_cb=EarlyStopping(monitor='val_loss', patience=50,verbose=1)\n",
        "cb = [model_cb, early_cb]\n",
        "# Load the entire model from the .h5 file\n",
        "model = load_model('/content/drive/MyDrive/Physics/NOAA/Model_NOAA.h5')\n",
        "history = model.fit(x_train,y_train,epochs=150,batch_size=32,verbose=1,callbacks=cb,shuffle=True,validation_data=[x_test, y_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-trained Vor-CNN"
      ],
      "metadata": {
        "id": "jrTJUlHVvRNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vorcnn = models.load_model('/content/drive/MyDrive/Physics/NOAA/Model_NOAA.h5')\n",
        "\n",
        "x_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_train.npy',mmap_mode = 'r')\n",
        "y_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_train.npy',mmap_mode = 'r')\n",
        "x_test = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_test.npy',mmap_mode = 'r')\n",
        "y_test = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_test.npy',mmap_mode = 'r')"
      ],
      "metadata": {
        "id": "TJYCndvRvVXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9uy8O5uH8uP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 进行预测\n",
        "res_test_cnn = vorcnn.predict(x_test)\n",
        "\n",
        "# 记录预测结束时间\n",
        "end_time = time.time()\n",
        "\n",
        "# 计算并打印预测所需的时间\n",
        "inference_time = end_time - start_time\n",
        "print(f\"Inference time: {inference_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "def calculate_metrics(original_images, predicted_images):\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "    l2_norms = []\n",
        "\n",
        "    for original, predicted in zip(original_images, predicted_images):\n",
        "        # Calculate SSIM\n",
        "        ssim_score = ssim(original, predicted, multichannel=True)\n",
        "        ssim_scores.append(ssim_score)\n",
        "\n",
        "        # Calculate PSNR\n",
        "        psnr_score = psnr(original, predicted, data_range=predicted.max() - predicted.min())\n",
        "        psnr_scores.append(psnr_score)\n",
        "\n",
        "        # Calculate L2 norm\n",
        "        l2_norm = np.linalg.norm(original - predicted)\n",
        "        l2_norms.append(l2_norm)\n",
        "\n",
        "    return ssim_scores, psnr_scores, l2_norms\n",
        "\n",
        "# Initialize lists to store metrics for each subdataset\n",
        "all_ssim_scores = []\n",
        "all_psnr_scores = []\n",
        "all_l2_norms = []\n",
        "\n",
        "# Calculate and print metrics for different subdatasets\n",
        "for i in range(len(y_test) // 1040):\n",
        "    y_subset = y_test[1040*i:1040*(i+1)]\n",
        "    res_subset = res_test_cnn[1040*i:1040*(i+1)]\n",
        "\n",
        "    # Calculate SSIM, PSNR, and L2 norm\n",
        "    ssim_scores, psnr_scores, l2_norms = calculate_metrics(y_subset, res_subset)\n",
        "\n",
        "    # Print average SSIM, PSNR, and L2 norm for the current subset\n",
        "    print(f\"Subset {i+1}:\")\n",
        "    print(f\"Average SSIM: {sum(ssim_scores) / len(ssim_scores)}\")\n",
        "    print(f\"Average PSNR: {sum(psnr_scores) / len(psnr_scores)}\")\n",
        "    print(f\"Average L2 Norm: {sum(l2_norms) / len(l2_norms)}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "YUSXDb8pNJ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(res_test_cnn[3])"
      ],
      "metadata": {
        "id": "UhU0IaCZwRjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('vorcnn.npy',res_test_cnn[3])"
      ],
      "metadata": {
        "id": "nIMQ2aSLwwXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw00zWDQPdGY"
      },
      "source": [
        "# CED-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsbHBRyK9A3y"
      },
      "outputs": [],
      "source": [
        "x_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_train.npy',mmap_mode = 'r')\n",
        "y_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_train.npy',mmap_mode = 'r')\n",
        "x_test = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_test.npy',mmap_mode = 'r')\n",
        "y_test = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_test.npy',mmap_mode = 'r')\n",
        "\n",
        "x_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_train.npy',mmap_mode = 'r')\n",
        "y_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_train.npy',mmap_mode = 'r')\n",
        "x_test = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_test.npy',mmap_mode = 'r')\n",
        "y_test = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_test.npy',mmap_mode = 'r')\n",
        "\n",
        "\n",
        "'''x_data = np.vstack([x_train,x_test])\n",
        "\n",
        "y_data = np.vstack([y_train,y_test])'''\n",
        "\n",
        "\n",
        "x_data = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA.npy',mmap_mode = 'r')\n",
        "y_data = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA.npy',mmap_mode = 'r')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbf3EjXw9FLE"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_NOAA.npy',x_data)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_NOAA.npy',y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aasFo5oCPfHM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "tf.keras.utils.set_random_seed(1)\n",
        "\n",
        "\n",
        "# Encoder\n",
        "input_img = layers.Input(shape=(180, 360, 1))\n",
        "x = layers.Conv2D(32, (7, 7), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(16, (7, 7), activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)  # Added to match dimensions correctly\n",
        "x = layers.Flatten()(x)\n",
        "encoded = layers.Dense(512, activation='relu', name='encoded')(x)\n",
        "\n",
        "# Decoder\n",
        "x = layers.Dense(45*90*16, activation='relu')(encoded)  # Adjusted to match the encoder output\n",
        "x = layers.Reshape((45, 90, 16))(x)\n",
        "x = layers.Conv2D(16, (7, 7), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(32, (7, 7), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(1, (7, 7), activation='linear', padding='same', name='decoded')(x)\n",
        "\n",
        "# Autoencoder Model\n",
        "autoencoder = models.Model(input_img, decoded)\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mae')\n",
        "autoencoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fnn_e_gSQewS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yOuziFQ0ewVp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PYz2qdOXKw6"
      },
      "outputs": [],
      "source": [
        "x_train = x_train[:,:,:,0]\n",
        "x_test = x_test[:,:,:,0]\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "tf.keras.utils.set_random_seed(12)\n",
        "\n",
        "\n",
        "# Callback to save the best model\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/Physics/NOAA/best_model_CED.h5',  # 保存模型的路径\n",
        "    monitor='val_loss',  # 监视的指标\n",
        "    save_best_only=True,  # 仅保存在验证集上性能最好的模型\n",
        "    verbose=1,  # 日志等级\n",
        "    mode='min',  # 监视指标的目标是最小化\n",
        "    save_format='h5'  # 模型保存格式\n",
        ")\n",
        "\n",
        "\n",
        "# Callbacks for adaptive learning rate, early stopping, and model checkpointing\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=40,\n",
        "    min_lr=1e-5\n",
        ")\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=60,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Training the autoencoder\n",
        "history = autoencoder.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=16,\n",
        "    callbacks=[reduce_lr, early_stop, model_checkpoint],  # Include model_checkpoint in the callbacks\n",
        "    validation_data=(x_test, y_test),\n",
        "    shuffle=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have already trained the autoencoder model\n",
        "\n",
        "# Get the encoder part\n",
        "encoder_input = autoencoder.input\n",
        "encoder_output = autoencoder.get_layer('encoded').output\n",
        "encoder = models.Model(encoder_input, encoder_output)\n",
        "\n",
        "# Get the decoder part\n",
        "decoder_input = layers.Input(shape=(512,))\n",
        "decoder_layers = autoencoder.layers[7:]  # Assuming the encoded layer is the 7th layer\n",
        "decoder_output = decoder_layers[0](decoder_input)  # Pass the input to the first layer of the decoder\n",
        "for layer in decoder_layers[1:]:\n",
        "    decoder_output = layer(decoder_output)  # Pass the output through each subsequent layer\n",
        "decoder = models.Model(decoder_input, decoder_output)\n"
      ],
      "metadata": {
        "id": "_5qPvdssQfz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSjzByJtV9I"
      },
      "source": [
        "## Load Pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIyhFcpyB5Es"
      },
      "outputs": [],
      "source": [
        "x_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_train.npy',mmap_mode = 'r')\n",
        "y_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_train.npy',mmap_mode = 'r')\n",
        "x_test = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_test.npy',mmap_mode = 'r')\n",
        "y_test = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_test.npy',mmap_mode = 'r')\n",
        "\n",
        "x_data = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA.npy',mmap_mode = 'r')\n",
        "y_data = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA.npy',mmap_mode = 'r')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPwpFOkVSlHN"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_NOAA.npy',x_data)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_NOAA.npy',y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s7qCdr1TEfB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUcr1vObnAtZ"
      },
      "outputs": [],
      "source": [
        "autoencoder = models.load_model('/content/drive/MyDrive/Physics/NOAA/best_model_CED.h5')\n",
        "\n",
        "encode_layers = autoencoder.layers[:7]\n",
        "\n",
        "# Create a new Sequential model with the extracted layers\n",
        "encoded = tf.keras.models.Sequential(encode_layers)\n",
        "\n",
        "\n",
        "last_layers = autoencoder.layers[-7:]\n",
        "new_model = tf.keras.models.Sequential(last_layers)\n",
        "#encoded_features = encoded.predict(x_data[:,:,:,0])\n",
        "reconstructed_images = autoencoder.predict(x_test[:,:,:,0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming encoded_features is obtained from the encoder_model\n",
        "encoded_features = encoder_model.predict(x_test[:,:,:,0])\n",
        "\n",
        "# Decode the encoded features using the custom DecoderLayer\n",
        "decoded_images_custom = decoder_layer(encoded_features[:10])\n",
        "\n",
        "# Reconstruct images directly from the autoencoder\n",
        "reconstructed_images_autoencoder = autoencoder.predict(x_test[:,:,:,0])\n",
        "\n"
      ],
      "metadata": {
        "id": "jJ32OhuMKUce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8qVQXU1TfqQ"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, decoder_model, **kwargs):\n",
        "        super(DecoderLayer, self).__init__(**kwargs)\n",
        "        # Ensure the decoder_model is a callable (e.g., a Keras model or a function)\n",
        "        if not callable(decoder_model):\n",
        "            raise ValueError(\"decoder_model must be callable\")\n",
        "        self.decoder_model = decoder_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # It's good practice to check if the inputs are valid for the decoder_model\n",
        "        # This can be more specific based on the expected input shape, type, etc.\n",
        "        if inputs is None:\n",
        "            raise ValueError(\"Input to DecoderLayer cannot be None\")\n",
        "\n",
        "        # Use decoder_model to decode the inputs\n",
        "        decoded_images = self.decoder_model(inputs)\n",
        "        return decoded_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnt3rG07Ti_l"
      },
      "source": [
        "## Plot NOAA CED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG5kjgU5zdPY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib as mpl\n",
        "\n",
        "# 调整全局字体大小和颜色条标签大小\n",
        "mpl.rcParams['font.size'] = 8\n",
        "mpl.rcParams['axes.labelsize'] = 6  # 提高了labelsize以提升可读性\n",
        "\n",
        "def plot_image_and_colorbar(ax, image, title, mask=None, cmap='viridis', cbar_labelsize=6, vmin=None, vmax=None):\n",
        "    # 如果提供了mask，将mask位置的值设置为NaN\n",
        "    if mask is not None:\n",
        "        image = np.where(mask, np.nan, image)\n",
        "    im = ax.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)  # 使用vmin和vmax\n",
        "    ax.set_title(title, fontsize=11)  # 增大标题字体\n",
        "    ax.axis('off')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    cbar = plt.colorbar(im, cax=cax)\n",
        "    cbar.ax.tick_params(labelsize=10)  # 减小刻度字体\n",
        "\n",
        "def plot_and_save_comparison_multi(true_images, vor_images, reconstructed_images, indices, dpi=300):\n",
        "    ncols = len(indices)\n",
        "    nrows = 4\n",
        "\n",
        "    # 确定颜色标尺范围\n",
        "    vmin = np.nanmin(true_images)\n",
        "    vmax = np.nanmax(true_images)\n",
        "    error_vmax = np.nanmax(np.abs(true_images - reconstructed_images))-25\n",
        "\n",
        "    fig, axs = plt.subplots(nrows, ncols, figsize=(3 * ncols, 6), dpi=dpi)\n",
        "\n",
        "    for col, index in enumerate(indices):\n",
        "        mask = true_images[index, :, :, 0] == 0\n",
        "\n",
        "        # 真实图像\n",
        "        plot_image_and_colorbar(axs[0, col], true_images[index, :, :, 0], \"(a) NOAA State Field\", mask=mask, vmin=vmin, vmax=vmax)\n",
        "\n",
        "        # 重建图像\n",
        "        plot_image_and_colorbar(axs[1, col], reconstructed_images[index, :, :, 0], \"(b) NOAA Reconstructed Fields\", mask=mask, vmin=vmin, vmax=vmax)\n",
        "\n",
        "        # 误差图\n",
        "        error_map = np.abs(true_images[index, :, :, 0] - reconstructed_images[index, :, :, 0])\n",
        "        plot_image_and_colorbar(axs[2, col], error_map, \"(c) Error Map\", mask=mask, cmap='viridis', vmin=0, vmax=error_vmax)\n",
        "\n",
        "        # Voronoi图像\n",
        "        plot_image_and_colorbar(axs[3, col], vor_images[index, :, :, 0], \"(d) NOAA Voronoi Fields\", vmin=vmin, vmax=vmax)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('CED-NOAA.png', dpi=dpi, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# 使用示例\n",
        "plot_and_save_comparison_multi(y_test, x_test, reconstructed_images, indices=[80, 200, 400, 800], dpi=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(reconstructed_images[1])"
      ],
      "metadata": {
        "id": "9X6cavEZozdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY2ORX5642RI"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import numpy as np\n",
        "\n",
        "def apply_mask_and_calculate_ssim_psnr(y_true, y_pred, mask):\n",
        "    # 在 y_pred 中应用 mask，mask 为 True 的地方设置为 0\n",
        "    y_pred_masked = np.where(mask, 0, y_pred)\n",
        "\n",
        "    # 直接计算 SSIM 和 PSNR\n",
        "    ssim_val = ssim(y_true, y_pred_masked, data_range=y_true.max() - y_true.min())\n",
        "    psnr_val = psnr(y_true, y_pred_masked, data_range=y_true.max() - y_true.min())\n",
        "\n",
        "    return ssim_val, psnr_val\n",
        "\n",
        "# 假设 y_test 和 reconstructed_images 已经是 np.array 形式\n",
        "ssim_values = []\n",
        "psnr_values = []\n",
        "\n",
        "# 这里假设 mask 的形状与 y_test 和 reconstructed_images 中的每张图像相同\n",
        "mask = y_test == 0  # 假设 mask 是根据 y_test 中的 0 值创建的\n",
        "\n",
        "for i in range(y_test.shape[0]):\n",
        "    ssim_val, psnr_val = apply_mask_and_calculate_ssim_psnr(y_test[i, :, :, 0], reconstructed_images[i, :, :, 0], mask[i, :, :, 0])\n",
        "    ssim_values.append(ssim_val)\n",
        "    psnr_values.append(psnr_val)\n",
        "\n",
        "# 计算平均 SSIM 和 PSNR\n",
        "mean_ssim = np.mean(ssim_values)\n",
        "mean_psnr = np.mean(psnr_values)\n",
        "\n",
        "print(f\"Average SSIM: {mean_ssim}\")\n",
        "print(f\"Average PSNR: {mean_psnr}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq6ssjuDTo2I"
      },
      "source": [
        "## LSTM Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krZCSFElnpKI"
      },
      "outputs": [],
      "source": [
        "encoded_features = encoded.predict(x_data[:,:,:,0])\n",
        "datasets = np.split(encoded_features, 18)\n",
        "\n",
        "# 随机选择数据集用作训练和测试\n",
        "train_indices, test_indices = train_test_split(np.arange(18), test_size=1/3, shuffle = False)#random_state=42\n",
        "\n",
        "# 准备训练和测试数据\n",
        "def prepare_data(indices, datasets, input_length=3, forecast_horizon=3):\n",
        "    X, y = [], []\n",
        "    for idx in indices:\n",
        "        data = datasets[idx]\n",
        "\n",
        "        for i in range(len(data) - input_length - forecast_horizon + 1):\n",
        "            X.append(data[i:(i + input_length)])\n",
        "            y.append(data[(i + input_length):(i + input_length + forecast_horizon)])\n",
        "    return np.array(X), np.array(y)\n",
        "x_train, y_train = prepare_data(train_indices, datasets)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_LSTM_train.npy',x_train)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_LSTM_train.npy',y_train)\n",
        "\n",
        "del x_train, y_train\n",
        "\n",
        "x_val, y_val = prepare_data(test_indices, datasets)\n",
        "\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_LSTM_val.npy',x_val)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_LSTM_val.npy',y_val)\n",
        "\n",
        "del x_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMgYL7gEZCJ4"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_NOAA_LSTM_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_LSTM_train.npy',mmap_mode = 'r')\n",
        "y_NOAA_LSTM_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_LSTM_train.npy',mmap_mode = 'r')\n",
        "x_NOAA_LSTM_val = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_LSTM_val.npy',mmap_mode = 'r')\n",
        "y_NOAA_LSTM_val = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_LSTM_val.npy',mmap_mode = 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWB1ac4eGt_z"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=40,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/Physics/NOAA/best_model_ori_NOAA_3steps_mae.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,        # Reduce learning rate by 20%\n",
        "    patience=30,       # Number of epochs with no improvement after which learning rate will be reduced\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    min_delta=0.0001,  # Minimum change to qualify as an improvement\n",
        "    cooldown=0,        # Number of epochs to wait before resuming normal operation after lr has been reduced\n",
        "    min_lr=0           # Lower bound on the learning rate\n",
        ")\n",
        "\n",
        "def build_optimized_model(n_step=3, feature_dim=512):\n",
        "    \"\"\"\n",
        "    Builds an optimized LSTM model with added regularization and adjusted complexity.\n",
        "\n",
        "    Parameters:\n",
        "    - n_step (int): The number of time steps per sequence.\n",
        "    - feature_dim (int): The dimensionality of the input features.\n",
        "\n",
        "    Returns:\n",
        "    - model (tf.keras.Model): The constructed LSTM-based model with optimizations.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=(n_step, feature_dim))\n",
        "    x = layers.LSTM(128, return_sequences=True)(inputs)  # Reduce LSTM units to balance model complexity\n",
        "    x = layers.LSTM(128)(x)\n",
        "    x = layers.RepeatVector(n_step)(x)\n",
        "    x = layers.TimeDistributed(layers.Dense(feature_dim))(x)\n",
        "    outputs = layers.Activation('relu')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=\"OptimizedModel\")\n",
        "    return model\n",
        "\n",
        "# Instantiate and compile the optimized model\n",
        "optimized_model = build_optimized_model()\n",
        "optimized_model.compile(\n",
        "    loss='mae',\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001)  # Adjusted learning rate\n",
        ")\n",
        "\n",
        "optimized_model.summary()\n",
        "\n",
        "best_val_loss = float('inf')  # Initialize best validation loss\n",
        "best_seed = None  # Initialize best seed\n",
        "\n",
        "# Loop through different random seeds\n",
        "for seed in range(5):  # Example: trying seeds from 0 to 9\n",
        "    tf.keras.backend.clear_session()  # Clear previous model to reset randomness\n",
        "    tf.random.set_seed(seed)  # Set random seed\n",
        "\n",
        "    # Rebuild and compile the model with the current seed\n",
        "    optimized_model = build_optimized_model()\n",
        "    optimized_model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "    # Train the model\n",
        "    history = optimized_model.fit(\n",
        "        x_NOAA_LSTM_train, y_NOAA_LSTM_train,\n",
        "        validation_data=(x_NOAA_LSTM_val, y_NOAA_LSTM_val),\n",
        "        epochs=150,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "    )\n",
        "\n",
        "    # Check if the current seed resulted in a better validation loss\n",
        "    if min(history.history['val_loss']) < best_val_loss:\n",
        "        best_val_loss = min(history.history['val_loss'])\n",
        "        best_seed = seed\n",
        "\n",
        "print(f\"Best seed: {best_seed}, Best validation loss: {best_val_loss}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-trained LSTM"
      ],
      "metadata": {
        "id": "aFnat5NWyLGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_NOAA_LSTM_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_LSTM_train.npy',mmap_mode = 'r')\n",
        "y_NOAA_LSTM_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_LSTM_train.npy',mmap_mode = 'r')\n",
        "x_NOAA_LSTM_val = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_LSTM_val.npy',mmap_mode = 'r')\n",
        "y_NOAA_LSTM_val = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_LSTM_val.npy',mmap_mode = 'r')"
      ],
      "metadata": {
        "id": "9Ldr2uJr0hQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = models.load_model('/content/drive/MyDrive/Physics/NOAA/best_model_ori_NOAA_3steps_mae.h5')\n"
      ],
      "metadata": {
        "id": "D0TbIig6yNEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_test_splitted = np.split(y_test, 6)\n",
        "\n",
        "y_test_cropped = [images[3:-2,:,:, :] for images in y_test_splitted]\n",
        "\n",
        "y_test_final = np.array(y_test_cropped)\n",
        "\n",
        "print(\"Final shape of y_test:\", y_test_final.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "inYqpRDA3on-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restruct_images_lstm.shape"
      ],
      "metadata": {
        "id": "q4-HFnvlItje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWXNs25ngBZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 可视化重建图像\n",
        "fig, axs = plt.subplots(3, 2, figsize=(10, 15))  # 创建子图布局\n",
        "\n",
        "# 计算 Colorbar 的范围\n",
        "vmin = y_test[0].min()\n",
        "vmax = y_test[0].max()\n",
        "\n",
        "for i in range(3):\n",
        "    # 绘制原始图像\n",
        "    im1 = axs[i, 0].imshow(reconstructed_images[i, :, :, 0], vmin=vmin, vmax=vmax)\n",
        "    axs[i, 0].set_title('Original Image {}'.format(i+1))  # 设置标题\n",
        "    axs[i, 0].axis('off')  # 关闭坐标轴\n",
        "\n",
        "    # 绘制LSTM重建图像\n",
        "    im2 = axs[i, 1].imshow(restruct_images_lstm[i, :, :, 0], vmin=vmin, vmax=vmax)\n",
        "    axs[i, 1].set_title('Reconstructed Image LSTM {}'.format(i+1))  # 设置标题\n",
        "    axs[i, 1].axis('off')  # 关闭坐标轴\n",
        "\n",
        "# 创建 Colorbar\n",
        "cbar1 = fig.colorbar(im1, ax=axs[:, 0])\n",
        "cbar2 = fig.colorbar(im2, ax=axs[:, 1])\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "cSOxT_TugB6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Bjllz4EgTze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsyQhT0uJR8z"
      },
      "outputs": [],
      "source": [
        "res_test_lst = lstm.predict(x_NOAA_LSTM_val)[0,:,:]\n",
        "decoder_layer = DecoderLayer(new_model)\n",
        "\n",
        "restruct_images_lstm = np.array(decoder_layer(res_test_lst[:1035,:]))\n",
        "restruct_images_true = np.array(decoder_layer(x_NOAA_LSTM_val[:,0,:][:10]))\n",
        "\n",
        "restruct_images_lstm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA0b5oOLKEq8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "def calculate_metrics(original_images, predicted_images):\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "    l2_norms = []\n",
        "\n",
        "    for original, predicted in zip(original_images, predicted_images):\n",
        "        # Calculate SSIM\n",
        "        ssim_score = ssim(original, predicted, multichannel=True)\n",
        "        ssim_scores.append(ssim_score)\n",
        "\n",
        "        # Calculate PSNR\n",
        "        psnr_score = psnr(original, predicted, data_range=predicted.max() - predicted.min())\n",
        "        psnr_scores.append(psnr_score)\n",
        "\n",
        "        # Calculate L2 norm\n",
        "        l2_norm = np.linalg.norm(original - predicted)\n",
        "        l2_norms.append(l2_norm)\n",
        "\n",
        "    return ssim_scores, psnr_scores, l2_norms\n",
        "\n",
        "# Calculate SSIM, PSNR, and L2 norm\n",
        "ssim_scores, psnr_scores, l2_norms = calculate_metrics(y_test_final[0,:,:,:,:], restruct_images_lstm)\n",
        "\n",
        "# Print average SSIM, PSNR, and L2 norm\n",
        "print(f\"Average SSIM: {sum(ssim_scores) / len(ssim_scores)}\")\n",
        "print(f\"Average PSNR: {sum(psnr_scores) / len(psnr_scores)}\")\n",
        "print(f\"Average L2 Norm: {sum(l2_norms) / len(l2_norms)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "def apply_mask(original_images, predicted_images):\n",
        "    masked_predicted_images = []\n",
        "    for original, predicted in zip(original_images, predicted_images):\n",
        "        # 创建一个掩码，其中原始图像的像素值为0的位置为True\n",
        "        mask = original == 0\n",
        "\n",
        "        # 使用掩码更新预测图像，将对应于掩码为True的位置的像素值设置为0\n",
        "        masked_predicted = np.where(mask, 0, predicted)\n",
        "        masked_predicted_images.append(masked_predicted)\n",
        "\n",
        "    return masked_predicted_images\n",
        "\n",
        "def calculate_metrics(original_images, predicted_images):\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "    l2_norms = []\n",
        "\n",
        "    for original, predicted in zip(original_images, predicted_images):\n",
        "        # Calculate SSIM\n",
        "        ssim_score = ssim(original, predicted, multichannel=True)\n",
        "        ssim_scores.append(ssim_score)\n",
        "\n",
        "        # Calculate PSNR\n",
        "        psnr_score = psnr(original, predicted, data_range=predicted.max() - predicted.min())\n",
        "        psnr_scores.append(psnr_score)\n",
        "\n",
        "        # Calculate L2 norm\n",
        "        l2_norm = np.linalg.norm(original - predicted)\n",
        "        l2_norms.append(l2_norm)\n",
        "\n",
        "    return ssim_scores, psnr_scores, l2_norms\n",
        "\n",
        "# 假设 y_test_final 是原始图像数组，restruct_images_lstm 是预测图像数组\n",
        "# 首先，将原始图像中的0值部分作为掩码应用到预测图像上\n",
        "masked_predicted_images = apply_mask(y_test_final[0,:,:,:,:], restruct_images_lstm)\n",
        "\n",
        "# 然后，计算更新后的预测图像与原始图像之间的SSIM、PSNR和L2 norm\n",
        "ssim_scores, psnr_scores, l2_norms = calculate_metrics(y_test_final[0,:,:,:,:], masked_predicted_images)\n",
        "\n",
        "# 打印平均SSIM、PSNR和L2 norm\n",
        "print(f\"Average SSIM: {np.mean(ssim_scores)}\")\n",
        "print(f\"Average PSNR: {np.mean(psnr_scores)}\")\n",
        "print(f\"Average L2 Norm: {np.mean(l2_norms)}\")\n"
      ],
      "metadata": {
        "id": "M_hZUogMHDvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.min(restruct_images_lstm[100])"
      ],
      "metadata": {
        "id": "-7wxJHd3F53I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics_subset(original_images, predicted_images):\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "    l2_norms = []\n",
        "\n",
        "    for original, predicted in zip(original_images, predicted_images):\n",
        "        # 创建一个掩码，其中原始图像的像素值为0的位置为True\n",
        "        mask = original == 0\n",
        "\n",
        "        # 使用掩码更新预测图像，将对应于掩码为True的位置的像素值设置为0\n",
        "        masked_predicted = np.where(mask, 0, predicted)\n",
        "\n",
        "        # Calculate SSIM\n",
        "        ssim_score = ssim(original, masked_predicted, multichannel=True)\n",
        "        ssim_scores.append(ssim_score)\n",
        "\n",
        "        # Calculate PSNR\n",
        "        psnr_score = psnr(original, masked_predicted, data_range=masked_predicted.max() - masked_predicted.min())\n",
        "        psnr_scores.append(psnr_score)\n",
        "\n",
        "        # Calculate L2 norm\n",
        "        l2_norm = np.linalg.norm(original - masked_predicted)\n",
        "        l2_norms.append(l2_norm)\n",
        "\n",
        "    return ssim_scores, psnr_scores, l2_norms\n",
        "import time\n",
        "\n",
        "# Initialize lists to store metrics for each subset\n",
        "all_ssim_scores = []\n",
        "all_psnr_scores = []\n",
        "all_l2_norms = []\n",
        "all_inference_times = []\n",
        "\n",
        "# Iterate over subsets of images and calculate metrics\n",
        "for i in range(len(y_test_final)):\n",
        "    # Calculate metrics for the current subset\n",
        "    start_time = time.time()\n",
        "    restruct_images_lstm = np.array(decoder_layer(res_test_lst[1035*i:1035*(i+1),:]))\n",
        "    res_test_lst = lstm.predict(x_NOAA_LSTM_val)[:,0,:]\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate inference time\n",
        "    inference_time = end_time - start_time\n",
        "    all_inference_times.append(inference_time)\n",
        "\n",
        "    # Calculate SSIM, PSNR, and L2 norms\n",
        "    ssim_scores, psnr_scores, l2_norms = calculate_metrics_subset(y_test_final[i], restruct_images_lstm)\n",
        "\n",
        "    # Append metrics to the lists\n",
        "    all_ssim_scores.append(ssim_scores)\n",
        "    all_psnr_scores.append(psnr_scores)\n",
        "    all_l2_norms.append(l2_norms)\n",
        "\n",
        "# Compute the average metrics for each subset\n",
        "avg_ssim_scores = [sum(scores) / len(scores) for scores in all_ssim_scores]\n",
        "avg_psnr_scores = [sum(scores) / len(scores) for scores in all_psnr_scores]\n",
        "avg_l2_norms = [sum(norms) / len(norms) for norms in all_l2_norms]\n",
        "avg_inference_time = sum(all_inference_times) / len(all_inference_times)\n",
        "\n",
        "# Print the average metrics for each subset\n",
        "for i in range(len(avg_ssim_scores)):\n",
        "    print(f\"Subset {i+1}:\")\n",
        "    print(f\"Average SSIM: {avg_ssim_scores[i]}\")\n",
        "    print(f\"Average PSNR: {avg_psnr_scores[i]}\")\n",
        "    print(f\"Average L2 Norm: {avg_l2_norms[i]}\")\n",
        "    print(f\"Inference Time: {avg_inference_time} seconds\")\n"
      ],
      "metadata": {
        "id": "n_5lCXz-Hxpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the data\n",
        "methods = ['CED-LSTM', 'Vor-CNN', 'ConvLSTM']  # Add ConvLSTM\n",
        "levels = [200, 240, 280, 300, 320, 340]\n",
        "ssim_ced_lstm = [0.8358804598332382, 0.835554236306358, 0.8490705987148428, 0.8525863788543473, 0.8454216991970055, 0.8443145120122972]  # Update with new data\n",
        "psnr_ced_lstm = [36.33874952578177, 35.59074896236341, 36.802145647367006, 37.13562049675428, 36.490809481591576, 36.826683817498804]  # Update with new data\n",
        "ssim_vor_cnn = [0.579636193, 0.598724861, 0.603436399, 0.61198152, 0.611980767, 0.61406588]\n",
        "psnr_vor_cnn = [29.09964257, 30.49640416, 30.62988655, 30.99013098, 30.95251542, 31.52378384]\n",
        "ssim_conv_lstm = [0.6215, 0.6545, 0.648, 0.6636, 0.6548, 0.6612]  # Add SSIM data for ConvLSTM\n",
        "psnr_conv_lstm = [27.8333, 29.0194, 29.1142, 30.2744, 29.1651, 30.4262]  # Add PSNR data for ConvLSTM\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot SSIM values\n",
        "plt.subplot(1, 2, 1)\n",
        "for method, ssim_scores, color, marker in zip(methods, [ssim_ced_lstm, ssim_vor_cnn, ssim_conv_lstm], ['blue', 'orange', 'green'], ['o', '^', 's']):\n",
        "    plt.plot(levels, ssim_scores, marker=marker, linestyle='-', color=color, label=method)\n",
        "plt.title('SSIM Comparison', fontsize=16)\n",
        "plt.xlabel('Level', fontsize=14)\n",
        "plt.ylabel('SSIM', fontsize=14)\n",
        "plt.xticks(levels)\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower right', fontsize='small')  # Decrease legend size\n",
        "\n",
        "# Plot PSNR values\n",
        "plt.subplot(1, 2, 2)\n",
        "for method, psnr_scores, color, marker in zip(methods, [psnr_ced_lstm, psnr_vor_cnn, psnr_conv_lstm], ['blue', 'orange', 'green'], ['o', '^', 's']):\n",
        "    plt.plot(levels, psnr_scores, marker=marker, linestyle='-', color=color, label=method)\n",
        "plt.title('PSNR Comparison', fontsize=16)\n",
        "plt.xlabel('Level', fontsize=14)\n",
        "plt.ylabel('PSNR', fontsize=14)\n",
        "plt.xticks(levels)\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower right', fontsize='small')  # Decrease legend size\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('cnn-CEDLSTM-ConvLSTM-compare.png', dpi=300)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LPCmG8GtO4Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "res_test_lst = lstm.predict(x_NOAA_LSTM_val)[:,0,:]\n",
        "\n",
        "end_time = time.time()\n",
        "# Calculate inference time\n",
        "inference_time = end_time - start_time\n",
        "inference_time"
      ],
      "metadata": {
        "id": "wQC9_4upKkv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('ced_lstm_4.npy',restruct_images_true[0])"
      ],
      "metadata": {
        "id": "nJtPfQqnXfcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jEnxtdGjXx0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(restruct_images_true[3])"
      ],
      "metadata": {
        "id": "BmUke37IxnRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvLSTM"
      ],
      "metadata": {
        "id": "VgvRb7onu8tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Normlization"
      ],
      "metadata": {
        "id": "XNVwwMWqtY9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_train.npy',mmap_mode = 'r')[:,:,:,0]\n",
        "y_train = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_train.npy',mmap_mode = 'r')\n",
        "x_test = np.load('/content/drive/MyDrive/Physics/Dataset10/x_NOAA_test.npy',mmap_mode = 'r')[:,:,:,0]\n",
        "y_test = np.load('/content/drive/MyDrive/Physics/Dataset10/y_NOAA_test.npy',mmap_mode = 'r')"
      ],
      "metadata": {
        "id": "LOhuVSIb9pdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_data(data):\n",
        "    # Initialize the scaled data array with the same shape as the input data\n",
        "    scaled_data = np.zeros_like(data, dtype=np.float32)\n",
        "    # Initialize arrays to store the min and max values for each sample\n",
        "    min_vals = np.zeros((data.shape[0], 1, 1, 1))\n",
        "    max_vals = np.zeros_like(min_vals)\n",
        "\n",
        "    # Iterate through each sample in the data\n",
        "    for i in range(data.shape[0]):\n",
        "        # Calculate the min and max values for the current sample\n",
        "        min_vals[i] = np.min(data[i])\n",
        "        max_vals[i] = np.max(data[i])\n",
        "        # Apply the scaling operation\n",
        "        scaled_data[i] = (data[i] - min_vals[i]) / (max_vals[i] - min_vals[i])\n",
        "\n",
        "    return scaled_data, min_vals, max_vals\n",
        "\n",
        "x_train_scaled,_,_ = scale_data(x_train)\n",
        "x_test_scaled,_,_ = scale_data(x_test)\n",
        "y_train_scaled,_,_ = scale_data(y_train)\n",
        "y_test_scaled,min_vals,max_vals = scale_data(y_test)\n"
      ],
      "metadata": {
        "id": "_CBkNvtIrIAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_scaled,min_vals,max_vals = scale_data(y_test)\n"
      ],
      "metadata": {
        "id": "oIQO5LPC4nOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vals[:20,:,:,:]"
      ],
      "metadata": {
        "id": "ezrqyY5y43_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_train_scaled.npy', x_train_scaled)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_test_scaled.npy', x_test_scaled)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_train_scaled.npy', y_train_scaled)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_test_scaled.npy', y_test_scaled)\n",
        "# 保存 min_vals 和 max_vals\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_test_min_vals.npy', min_vals)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_test_max_vals.npy', max_vals)\n"
      ],
      "metadata": {
        "id": "3y4duD954inQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载数据\n",
        "x_train_scaled = np.load('/content/drive/MyDrive/Physics/Dataset10/x_train_scaled.npy', mmap_mode='r+')\n",
        "x_test_scaled = np.load('/content/drive/MyDrive/Physics/Dataset10/x_test_scaled.npy', mmap_mode='r+')\n",
        "y_train_scaled = np.load('/content/drive/MyDrive/Physics/Dataset10/y_train_scaled.npy', mmap_mode='r+')\n",
        "y_test_scaled = np.load('/content/drive/MyDrive/Physics/Dataset10/y_test_scaled.npy', mmap_mode='r+')\n",
        "x_train_scaled = x_train_scaled.reshape(x_train_scaled.shape[0], 180, 360, 1)\n",
        "x_test_scaled = x_test_scaled.reshape(x_test_scaled.shape[0], 180, 360, 1)\n",
        "\n",
        "\n",
        "# 加载 min_vals 和 max_vals\n",
        "min_vals = np.load('/content/drive/MyDrive/Physics/Dataset10/y_test_min_vals.npy')\n",
        "max_vals = np.load('/content/drive/MyDrive/Physics/Dataset10/y_test_max_vals.npy')\n"
      ],
      "metadata": {
        "id": "r3wBfTaA5GH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def prepare_convLSTM_data(x_data, y_data, num_of_sources, input_length=3, forecast_horizon=3):\n",
        "    X, y = [], []\n",
        "\n",
        "    # 计算每个源的样本数量\n",
        "    samples_per_source = len(x_data) // num_of_sources\n",
        "\n",
        "    for i in range(num_of_sources):\n",
        "        # 根据每个源的样本数量计算起始和结束索引\n",
        "        start_idx = i * samples_per_source\n",
        "        end_idx = start_idx + samples_per_source\n",
        "\n",
        "        # 切分x_data和y_data，获取当前源的数据\n",
        "        x_source_data = x_data[start_idx:end_idx]\n",
        "        y_source_data = y_data[start_idx:end_idx]\n",
        "\n",
        "        # 将每个样本的源数量合并成一个新的维度\n",
        "        x_sample = x_source_data.reshape(samples_per_source, 180,360,1)\n",
        "        y_sample = y_source_data.reshape(samples_per_source, 180,360,1)\n",
        "\n",
        "        # 将每个样本划分为输入和输出序列\n",
        "        for j in range(samples_per_source - input_length - forecast_horizon + 1):\n",
        "            X.append(x_sample[j:j+input_length])\n",
        "            y.append(y_sample[j+input_length:j+input_length+forecast_horizon])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 定义输入和输出长度\n",
        "input_length = 3\n",
        "forecast_horizon = 3\n",
        "\n",
        "# 计算源的数量\n",
        "num_of_sources = len(x_train_scaled) // 1040\n",
        "\n",
        "# 准备ConvLSTM训练数据集\n",
        "'''x_convLSTM_train, y_convLSTM_train = prepare_convLSTM_data(x_train_scaled, y_train_scaled, num_of_sources, input_length, forecast_horizon)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_convLSTM_train.npy', x_convLSTM_train)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_convLSTM_train.npy', y_convLSTM_train)\n",
        "print(\"x_convLSTM_train shape:\", x_convLSTM_train.shape)\n",
        "print(\"y_convLSTM_train shape:\", y_convLSTM_train.shape)\n",
        "del x_convLSTM_train,y_convLSTM_train'''\n",
        "# 准备ConvLSTM测试数据集\n",
        "x_convLSTM_test, y_convLSTM_test = prepare_convLSTM_data(x_test_scaled, y_test_scaled, num_of_sources, input_length, forecast_horizon)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/x_convLSTM_test.npy', x_convLSTM_test)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_convLSTM_test.npy', y_convLSTM_test)\n",
        "print(\"x_convLSTM_test shape:\", x_convLSTM_test.shape)\n",
        "print(\"y_convLSTM_test shape:\", y_convLSTM_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "YOyxjzne-w-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def prepare_convLSTM_data(min_vals, max_vals, num_of_sources, input_length=3, forecast_horizon=3):\n",
        "    X, y = [], []\n",
        "\n",
        "    # 计算每个源的样本数量\n",
        "    samples_per_source = len(min_vals) // num_of_sources\n",
        "\n",
        "    for i in range(num_of_sources):\n",
        "        # 根据每个源的样本数量计算起始和结束索引\n",
        "        start_idx = i * samples_per_source\n",
        "        end_idx = start_idx + samples_per_source\n",
        "\n",
        "        # 切分x_data和y_data，获取当前源的数据\n",
        "        x_source_data = min_vals[start_idx:end_idx]\n",
        "        y_source_data = max_vals[start_idx:end_idx]\n",
        "\n",
        "        # 将每个样本的源数量合并成一个新的维度\n",
        "        x_sample = x_source_data.reshape(samples_per_source, 1,1,1)\n",
        "        y_sample = y_source_data.reshape(samples_per_source, 1,1,1)\n",
        "\n",
        "        # 将每个样本划分为输入和输出序列\n",
        "        for j in range(samples_per_source - input_length - forecast_horizon + 1):\n",
        "            X.append(x_sample[j+input_length:j+input_length+forecast_horizon])\n",
        "            y.append(y_sample[j+input_length:j+input_length+forecast_horizon])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 定义输入和输出长度\n",
        "input_length = 3\n",
        "forecast_horizon = 3\n",
        "\n",
        "# 计算源的数量\n",
        "num_of_sources = len(x_train_scaled) // 1040\n",
        "min_vals, max_vals\n",
        "# 准备ConvLSTM训练数据集\n",
        "min_vals, max_vals = prepare_convLSTM_data(min_vals, max_vals, num_of_sources, input_length, forecast_horizon)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/min_vals_convlstm.npy', min_vals)\n",
        "np.save('/content/drive/MyDrive/Physics/Dataset10/max_vals_convlstm.npy', max_vals)\n",
        "print(\"x_convLSTM_train shape:\", min_vals.shape)\n",
        "print(\"y_convLSTM_train shape:\", max_vals.shape)\n"
      ],
      "metadata": {
        "id": "QQCySy7Rg5Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "J2GHlPeBOnSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load all scaled data\n",
        "x_convLSTM_train_scaled = np.load('/content/drive/MyDrive/Physics/Dataset10/x_convLSTM_train.npy', mmap_mode='r')\n",
        "y_convLSTM_train_scaled = np.load('/content/drive/MyDrive/Physics/Dataset10/y_convLSTM_train.npy', mmap_mode='r')\n",
        "x_convLSTM_test_scaled = np.load('/content/drive/MyDrive/Physics/Dataset10/x_convLSTM_test.npy', mmap_mode='r')\n",
        "y_convLSTM_test_scaled = np.load('/content/drive/MyDrive/Physics/Dataset10/y_convLSTM_test.npy', mmap_mode='r')\n",
        "min_vals_convlstm = np.load('/content/drive/MyDrive/Physics/Dataset10/min_vals_convlstm.npy', mmap_mode='r')\n",
        "max_vals_convlstm = np.load('/content/drive/MyDrive/Physics/Dataset10/max_vals_convlstm.npy', mmap_mode='r')\n",
        "\n",
        "y_convLSTM_test_restored = np.load('/content/drive/MyDrive/Physics/Dataset10/y_convLSTM_test_restored.npy', mmap_mode='r')\n",
        "\n",
        "# Access the loaded data\n",
        "print(\"x_convLSTM_train_scaled shape:\", x_convLSTM_train_scaled.shape)\n",
        "print(\"y_convLSTM_train_scaled shape:\", y_convLSTM_train_scaled.shape)\n",
        "print(\"x_convLSTM_test_scaled shape:\", x_convLSTM_test_scaled.shape)\n",
        "print(\"y_convLSTM_test_scaled shape:\", y_convLSTM_test_scaled.shape)\n"
      ],
      "metadata": {
        "id": "Z7JTzLresgjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restore_original_scale(scaled_data, min_vals, max_vals):\n",
        "    # Ensure min_vals and max_vals are broadcastable to the shape of scaled_data\n",
        "    # This might require reshaping min_vals and max_vals depending on how they were saved\n",
        "    restored_data = scaled_data * (max_vals - min_vals) + min_vals\n",
        "    return restored_data"
      ],
      "metadata": {
        "id": "saDV9riRduxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/Physics/Dataset10/y_convLSTM_test_restored.npy', y_convLSTM_test_restored)\n"
      ],
      "metadata": {
        "id": "vR_0WXUUgjMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (None, 3, 180, 360, 1)\n",
        "\n",
        "# Input layer\n",
        "inputs = layers.Input(shape=input_shape[1:])\n",
        "\n",
        "# ConvLSTM layers\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=16,\n",
        "    kernel_size=(7, 7),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(inputs)\n",
        "\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=16,\n",
        "    kernel_size=(7, 7),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=16,\n",
        "    kernel_size=(7, 7),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "\n",
        "# Output layer\n",
        "outputs = layers.Conv3D(\n",
        "    filters=1,\n",
        "    kernel_size=(7, 7, 7),\n",
        "    activation=\"linear\",\n",
        "    padding=\"same\"\n",
        ")(x)\n",
        "\n",
        "# Create the model\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.mean_absolute_error, optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ZivecE1Wu-oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n"
      ],
      "metadata": {
        "id": "NzsAKk_oQVlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "'''# Set the random seed for reproducibility\n",
        "tf.keras.utils.set_random_seed(12)'''\n",
        "#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "model = models.load_model('/content/drive/MyDrive/Physics/NOAA/best_model_convlstm_retrain.h5')\n",
        "\n",
        "# Callback to save the best model\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/Physics/NOAA/best_model_convlstm_retrain.h5',  # 保存模型的路径\n",
        "    monitor='val_loss',  # 监视的指标\n",
        "    save_best_only=True,  # 仅保存在验证集上性能最好的模型\n",
        "    verbose=1,  # 日志等级\n",
        "    mode='min',  # 监视指标的目标是最小化\n",
        "    save_format='h5'  # 模型保存格式\n",
        ")\n",
        "\n",
        "\n",
        "# Callbacks for adaptive learning rate, early stopping, and model checkpointing\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=30,\n",
        "    min_lr=1e-5\n",
        ")\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=45,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Training the autoencoder\n",
        "history = model.fit(\n",
        "    x_convLSTM_train_scaled,\n",
        "    y_convLSTM_train_scaled,\n",
        "    epochs=10,\n",
        "    batch_size=4,\n",
        "    callbacks=[reduce_lr, model_checkpoint],  # Include model_checkpoint in the callbacks\n",
        "    validation_data=(x_convLSTM_test_scaled, y_convLSTM_test_scaled),\n",
        "    shuffle=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "EucN6xeepukw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Physics/NOAA/best_model_convlstm_retrain.h5')"
      ],
      "metadata": {
        "id": "1-GtspHg6Vxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the autoencoder\n",
        "history = model.fit(\n",
        "    x_convLSTM_train_scaled,\n",
        "    y_convLSTM_train_scaled,\n",
        "    epochs=50,\n",
        "    batch_size=8,\n",
        "    callbacks=[reduce_lr, model_checkpoint],  # Include model_checkpoint in the callbacks\n",
        "    validation_data=(x_convLSTM_test_scaled, y_convLSTM_test_scaled),\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "d6lDBbSEydAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0_Ehp2IaVTz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.load_model('/content/drive/MyDrive/Physics/NOAA/best_model_convlstm_final.h5')\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/Physics/NOAA/best_model_convlstm_final.h5',  # 保存模型的路径\n",
        "    monitor='val_loss',  # 监视的指标\n",
        "    save_best_only=True,  # 仅保存在验证集上性能最好的模型\n",
        "    verbose=1,  # 日志等级\n",
        "    mode='min',  # 监视指标的目标是最小化\n",
        "    save_format='h5'  # 模型保存格式\n",
        ")\n",
        "\n",
        "\n",
        "# Callbacks for adaptive learning rate, early stopping, and model checkpointing\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=30,\n",
        "    min_lr=1e-5\n",
        ")\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=45,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Training the autoencoder\n",
        "history = model.fit(\n",
        "    x_convLSTM_train_scaled,\n",
        "    y_convLSTM_train_scaled,\n",
        "    epochs=10,\n",
        "    batch_size=4,\n",
        "    callbacks=[reduce_lr, model_checkpoint],  # Include model_checkpoint in the callbacks\n",
        "    validation_data=(x_convLSTM_test_scaled, y_convLSTM_test_scaled),\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "_pE1M0V-ZQYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Enable mixed precision training\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "tf.random.set_seed(12)\n",
        "\n",
        "# Callback to save the best model\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/Physics/NOAA/best_model_convlstm.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Callbacks for adaptive learning rate, early stopping, and model checkpointing\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=40,\n",
        "    min_lr=1e-5\n",
        ")\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=60,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Build and compile your model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    x_convLSTM_train,\n",
        "    y_convLSTM_train,\n",
        "    epochs=150,\n",
        "    batch_size=4,\n",
        "    callbacks=[reduce_lr, early_stop, model_checkpoint],\n",
        "    validation_data=(x_convLSTM_test, y_convLSTM_test),\n",
        "    shuffle=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "QLJY9qSGijZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Performance"
      ],
      "metadata": {
        "id": "kKXMp9USZ4QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_convLSTM_train_scaled.shape"
      ],
      "metadata": {
        "id": "SuDNq0ADBuC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.load_model('/content/drive/MyDrive/Physics/NOAA/best_model_convlstm_32units.h5')\n",
        "\n",
        "res_lstm = model.predict(x_convLSTM_test_scaled)\n",
        "\n",
        "res_lstm_restored = restore_original_scale(res_lstm, min_vals_convlstm, max_vals_convlstm)\n"
      ],
      "metadata": {
        "id": "IXzrAcUx_Ra0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "num_sensors = [200, 240, 280, 300, 320, 340]\n",
        "ssim = [0.6215, 0.6545, 0.648, 0.6636, 0.6548, 0.6612]\n",
        "psnr = [27.8333, 29.02, 29.1142, 30.2744, 29.1651, 30.4262]\n",
        "\n",
        "# Plotting SSIM\n",
        "plt.figure(figsize=(10, 3))  # 调整图的宽度\n",
        "plt.plot(num_sensors, ssim, color='blue', marker='o', label='SSIM')\n",
        "plt.xlabel('Number of Sensors', fontsize=14)\n",
        "plt.ylabel('SSIM', fontsize=14)\n",
        "plt.title('SSIM vs. Number of Sensors')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plotting PSNR\n",
        "plt.figure(figsize=(10, 3))  # 调整图的宽度\n",
        "plt.plot(num_sensors, psnr, color='orange', marker='o', label='PSNR')\n",
        "plt.xlabel('Number of Sensors', fontsize=14)\n",
        "plt.ylabel('PSNR', fontsize=14)\n",
        "plt.title('PSNR vs. Number of Sensors')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NfZAtCGSKIAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "num_sensors = [200, 240, 280, 300, 320, 340]\n",
        "ssim = [0.6215, 0.6545, 0.648, 0.6636, 0.6548, 0.6612]\n",
        "psnr = [27.8333, 29.02, 29.1142, 30.2744, 29.1651, 30.4262]\n",
        "\n",
        "# Plotting SSIM and PSNR together\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5))  # 调整图的大小\n",
        "ax1.plot(num_sensors, ssim, color='blue', marker='o', label='SSIM')\n",
        "ax1.set_xlabel('Number of Sensors', fontsize=14)  # 调整横坐标标签的字体大小\n",
        "ax1.set_ylabel('SSIM', color='blue', fontsize=14)  # 调整纵坐标标签的字体大小\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax1.tick_params(axis='both', labelsize=12)  # 调整横纵坐标数字的字体大小\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2 = ax1.twinx()  # 创建第二个坐标轴\n",
        "ax2.plot(num_sensors, psnr, color='orange', marker='o', label='PSNR')\n",
        "ax2.set_ylabel('PSNR', color='orange', fontsize=14)  # 调整纵坐标标签的字体大小\n",
        "ax2.tick_params(axis='y', labelcolor='orange')\n",
        "ax2.tick_params(axis='both', labelsize=12)  # 调整横纵坐标数字的字体大小\n",
        "\n",
        "plt.title('SSIM and PSNR vs. Number of Sensors', fontsize=16)  # 调整标题字体大小\n",
        "\n",
        "fig.tight_layout()  # 调整布局以防止重叠\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "gWiC3N82ZSwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thf = res_lstm[-1030:]\n",
        "thf_input = x_convLSTM_test_scaled[-1030:]"
      ],
      "metadata": {
        "id": "z9T6N3ISJ96s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_lstm_train = model.predict(x_convLSTM_train_scaled[:1])\n"
      ],
      "metadata": {
        "id": "BRWhNxSMBmpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the vmin and vmax for both datasets\n",
        "vmin = np.min(x_convLSTM_test_scaled[0])#, np.min(x_convLSTM_train_scaled[:1]))\n",
        "vmax = np.max(x_convLSTM_test_scaled[0])#, np.max(x_convLSTM_train_scaled[:1]))\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Plotting the first row (res_lstm_train)\n",
        "for i in range(3):\n",
        "    im1 = axs[0, i].imshow(res_lstm_train[0, i].reshape((180, 360)), cmap='viridis', vmin=vmin, vmax=vmax)\n",
        "    axs[0, i].set_title(f\"res_lstm_train - Slice {i+1}\")\n",
        "    axs[0, i].axis('off')\n",
        "\n",
        "# Plotting the second row (x_convLSTM_train_scaled)\n",
        "for i in range(3):\n",
        "    im2 = axs[1, i].imshow(x_convLSTM_test_scaled[:1][0, i].reshape((180, 360)), cmap='viridis', vmin=vmin, vmax=vmax)\n",
        "    axs[1, i].set_title(f\"x_convLSTM_train_scaled - Slice {i+1}\")\n",
        "    axs[1, i].axis('off')\n",
        "\n",
        "# Add colorbars with the same range\n",
        "cbar_ax1 = fig.add_axes([0.92, 0.6, 0.02, 0.3])  # Position for colorbar of the first row\n",
        "cbar1 = fig.colorbar(im1, cax=cbar_ax1)\n",
        "cbar_ax2 = fig.add_axes([0.92, 0.1, 0.02, 0.3])  # Position for colorbar of the second row\n",
        "cbar2 = fig.colorbar(im2, cax=cbar_ax2)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "UQZCWrmiB11p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_lstm_conv = res_lstm_restored[:,0,:,:,:]\n",
        "\n",
        "true_state = y_convLSTM_test_restored[:,0,:,:,:]"
      ],
      "metadata": {
        "id": "yF2MBLKrfCCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('convlstm_4.npy',res_lstm_conv[0])"
      ],
      "metadata": {
        "id": "b3M9_hYsZchL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W7fAspLDZ_y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_mask(original_images, predicted_images):\n",
        "    masked_predicted_images = []\n",
        "    for original, predicted in zip(original_images, predicted_images):\n",
        "        # 创建一个掩码，其中原始图像的像素值为0的位置为True\n",
        "        mask = original == 0\n",
        "\n",
        "        # 使用掩码更新预测图像，将对应于掩码为True的位置的像素值设置为0\n",
        "        masked_predicted = np.where(mask, 0, predicted)\n",
        "        masked_predicted_images.append(masked_predicted)\n",
        "\n",
        "    return masked_predicted_images"
      ],
      "metadata": {
        "id": "6Q7trgOBbmdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(res_lstm_conv[1000,:,:,:])"
      ],
      "metadata": {
        "id": "ItjLAJgqcfpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import time\n",
        "\n",
        "# Initialize lists to store results\n",
        "ssim_scores = []\n",
        "psnr_scores = []\n",
        "inference_times = []\n",
        "\n",
        "# Define the number of cases\n",
        "num_cases = 6\n",
        "\n",
        "# Calculate the size of each case\n",
        "case_size = len(res_lstm) // num_cases\n",
        "\n",
        "# Loop over each case\n",
        "for i in range(num_cases):\n",
        "    # Get the subset of data for the current case\n",
        "    res_subset = res_lstm_conv[i * case_size : (i + 1) * case_size]\n",
        "    true_subset = true_state[i * case_size : (i + 1) * case_size]\n",
        "    res_subset = apply_mask(true_subset, res_subset)\n",
        "    # Initialize lists to store metrics for the current case\n",
        "    case_ssim_scores = []\n",
        "    case_psnr_scores = []\n",
        "    case_inference_times = []\n",
        "\n",
        "    # Calculate metrics for each sample in the current case\n",
        "    for res, true in zip(res_subset, true_subset):\n",
        "        # Measure inference time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Calculate SSIM\n",
        "        ssim_score = ssim(res, true, multichannel=True)\n",
        "        case_ssim_scores.append(ssim_score)\n",
        "\n",
        "        # Calculate PSNR\n",
        "        psnr_score = psnr(res, true, data_range=res.max() - res.min())\n",
        "        case_psnr_scores.append(psnr_score)\n",
        "\n",
        "        # Measure inference time\n",
        "        end_time = time.time()\n",
        "        inference_time = end_time - start_time\n",
        "        case_inference_times.append(inference_time)\n",
        "\n",
        "    # Calculate the average SSIM, PSNR, and inference time for the current case\n",
        "    avg_ssim_score = sum(case_ssim_scores) / len(case_ssim_scores)\n",
        "    avg_psnr_score = sum(case_psnr_scores) / len(case_psnr_scores)\n",
        "    avg_inference_time = sum(case_inference_times) / len(case_inference_times)\n",
        "\n",
        "    # Append the results to the lists\n",
        "    ssim_scores.append(avg_ssim_score)\n",
        "    psnr_scores.append(avg_psnr_score)\n",
        "    inference_times.append(avg_inference_time)\n",
        "\n",
        "# Print the results for each case\n",
        "for i in range(num_cases):\n",
        "    print(f\"Case {i+1}:\")\n",
        "    print(f\"Average SSIM: {ssim_scores[i]}\")\n",
        "    print(f\"Average PSNR: {psnr_scores[i]}\")\n",
        "    print(f\"Average Inference Time: {inference_times[i]} seconds\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "PwEhgY5rBuHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Sample"
      ],
      "metadata": {
        "id": "UO9thgphqHC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TD_kriging = np.load('/content/2D_result_4-6.npy')[0]\n",
        "tD_kriging = np.load('/content/3D_result_4-6.npy')[0]\n",
        "\n",
        "TD_kriging_rotated = np.rot90(TD_kriging, k=-1)\n",
        "tD_kriging_rotated = np.rot90(tD_kriging, k=-1)\n",
        "\n",
        "vorcnn = np.load('/content/vorcnn.npy')\n",
        "convlstm= np.load('/content/convlstm_4.npy')\n",
        "ced = np.load('/content/ced_lstm_4.npy')\n",
        "\n",
        "true_field = true_state[0]"
      ],
      "metadata": {
        "id": "2ftm3PlFaArL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of TD_kriging:\", TD_kriging_rotated.shape)\n",
        "print(\"Shape of tD_kriging:\", tD_kriging_rotated.shape)\n",
        "print(\"Shape of vorcnn:\", vorcnn.shape)\n",
        "print(\"Shape of convlstm:\", convlstm.shape)\n",
        "print(\"Shape of ced:\", ced.shape)\n",
        "print(\"Shape of true:\", true_field.shape)\n"
      ],
      "metadata": {
        "id": "RgyblL-7a3PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib as mpl\n",
        "\n",
        "mpl.rcParams['font.size'] = 10\n",
        "mpl.rcParams['axes.labelsize'] = 8\n",
        "\n",
        "error_TD_kriging = true_field - TD_kriging_rotated.reshape((180,360,1))\n",
        "\n",
        "# Calculate the errors for tD_kriging\n",
        "error_tD_kriging = true_field - tD_kriging_rotated.reshape((180,360,1))\n",
        "\n",
        "\n",
        "# Calculate the errors for vorcnn\n",
        "error_vorcnn = true_field - vorcnn\n",
        "\n",
        "# Calculate the errors for convlstm\n",
        "error_convlstm = true_field - convlstm\n",
        "\n",
        "# Calculate the errors for ced\n",
        "error_ced = true_field - ced\n",
        "\n",
        "abs_error_TD_kriging = np.abs(error_TD_kriging)\n",
        "\n",
        "abs_error_tD_kriging = np.abs(error_tD_kriging)\n",
        "\n",
        "abs_error_vorcnn = np.abs(error_vorcnn)\n",
        "\n",
        "abs_error_convlstm = np.abs(error_convlstm)\n",
        "abs_error_ced = np.abs(error_ced)\n",
        "\n",
        "\n",
        "def plot_image_and_colorbar(ax, image, title, mask=None, cmap='viridis', cbar_labelsize=8, vmin=None, vmax=None):\n",
        "    if mask is not None:\n",
        "        image = np.ma.array(image, mask=mask)  # 使用掩码数组而不是直接修改数据\n",
        "    im = ax.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    cbar = plt.colorbar(im, cax=cax)\n",
        "    cbar.ax.tick_params(labelsize=cbar_labelsize)  # 设置颜色条刻度的字体大小\n",
        "\n",
        "# 假设true, TD_kriging_rotated等变量已经加载\n",
        "mask = (y_test[0] == 0)  # 定义掩码\n",
        "\n",
        "fig, axs = plt.subplots(4, 3, figsize=(18, 24), constrained_layout=True)\n",
        "images = [true_field, TD_kriging_rotated, tD_kriging_rotated, vorcnn, convlstm, ced]\n",
        "errors = [abs_error_TD_kriging, abs_error_tD_kriging, abs_error_vorcnn, abs_error_convlstm, abs_error_ced]\n",
        "titles = ['True Field', '2D Kriging', '3D Kriging', '2D Kriging Error Map', '3D Kriging Error Map', 'Vor-CNN', 'Conv LSTM', 'CED-LSTM',\n",
        "          'Vor-CNN Error Map', 'Conv LSTM Error Map', 'CED-LSTM Error Map']\n",
        "cmaps = [cmap_viridis] * 6 + [cmap_hot] * 5\n",
        "\n",
        "# 确定颜色范围\n",
        "vmin, vmax = np.nanmin(true_field), np.nanmax(true_field)\n",
        "error_vmin, error_vmax = 0, np.nanmax(abs_error_convlstm)\n",
        "\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    if i < 6:\n",
        "        plot_image_and_colorbar(ax, images[i], titles[i], mask=mask, cmap=cmaps[i], vmin=vmin, vmax=vmax)\n",
        "    elif i < 11:\n",
        "        plot_image_and_colorbar(ax, errors[i-6], titles[i], mask=mask, cmap=cmaps[i], vmin=error_vmin, vmax=error_vmax)\n",
        "    else:\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LedooeBRqwoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "mpl.rcParams['font.size'] = 10\n",
        "mpl.rcParams['axes.labelsize'] = 8\n",
        "\n",
        "# Calculate errors\n",
        "error_TD_kriging = true_field - TD_kriging_rotated.reshape((180,360,1))\n",
        "error_tD_kriging = true_field - tD_kriging_rotated.reshape((180,360,1))\n",
        "error_vorcnn = true_field - vorcnn\n",
        "error_convlstm = true_field - convlstm\n",
        "error_ced = true_field - ced\n",
        "\n",
        "abs_error_TD_kriging = np.abs(error_TD_kriging)\n",
        "abs_error_tD_kriging = np.abs(error_tD_kriging)\n",
        "abs_error_vorcnn = np.abs(error_vorcnn)\n",
        "abs_error_convlstm = np.abs(error_convlstm)\n",
        "abs_error_ced = np.abs(error_ced)\n",
        "\n",
        "mask = (y_test[0]==0)\n",
        "# Plotting functions\n",
        "def plot_image_and_colorbar(ax, image, title, cmap='viridis', vmin=None, vmax=None):\n",
        "    if mask is not None:\n",
        "        image = np.ma.array(image, mask=mask)\n",
        "    im = ax.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    cbar = plt.colorbar(im, cax=cax)\n",
        "    cbar.ax.tick_params(labelsize=8)\n",
        "    return cbar\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(4, 3, figsize=(18, 24), constrained_layout=True)\n",
        "\n",
        "# True Field, 3D Kriging, 2D Kriging\n",
        "cbar1 = plot_image_and_colorbar(axs[0, 0], true_field, 'True Field')\n",
        "plot_image_and_colorbar(axs[0, 1], TD_kriging_rotated, '3D Kriging', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[0, 2], tD_kriging_rotated, '2D Kriging', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "\n",
        "# Empty spaces\n",
        "axs[1, 0].axis('off')\n",
        "\n",
        "# Error maps for 3D Kriging and 2D Kriging\n",
        "cbar2 = plot_image_and_colorbar(axs[1, 1], abs_error_TD_kriging, 'Error Map (3D Kriging)', vmin=cbar2.vmin, vmax=cbar2.vmax)\n",
        "plot_image_and_colorbar(axs[1, 2], abs_error_tD_kriging, 'Error Map (2D Kriging)', vmin=cbar2.vmin, vmax=cbar2.vmax)\n",
        "\n",
        "# Vor-CNN, ConvLSTM, CED-LSTM\n",
        "plot_image_and_colorbar(axs[2, 0], vorcnn, 'Vor-CNN', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[2, 1], convlstm, 'ConvLSTM', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[2, 2], ced, 'CED-LSTM', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "\n",
        "# Empty spaces\n",
        "cbar3 = plot_image_and_colorbar(axs[3, 0], abs_error_vorcnn, 'Error Map (Vor-CNN)', vmin=cbar2.vmin, vmax=cbar2.vmax)\n",
        "\n",
        "# Error maps for Vor-CNN, ConvLSTM, CED-LSTM\n",
        "plot_image_and_colorbar(axs[3, 1], abs_error_convlstm, 'Error Map (ConvLSTM)', vmin=cbar2.vmin, vmax=cbar2.vmax)\n",
        "cbar4 = plot_image_and_colorbar(axs[3, 2], abs_error_ced, 'Error Map (CED-LSTM)', vmin=cbar2.vmin, vmax=cbar2.vmax)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B2TRtX7KrZiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "mpl.rcParams['font.size'] = 10\n",
        "mpl.rcParams['axes.labelsize'] = 8\n",
        "\n",
        "# Calculate errors\n",
        "error_TD_kriging = true_field - TD_kriging_rotated.reshape((180,360,1))\n",
        "error_tD_kriging = true_field - tD_kriging_rotated.reshape((180,360,1))\n",
        "error_vorcnn = true_field - vorcnn\n",
        "error_convlstm = true_field - convlstm\n",
        "error_ced = true_field - ced\n",
        "\n",
        "abs_error_TD_kriging = np.abs(error_TD_kriging)\n",
        "abs_error_tD_kriging = np.abs(error_tD_kriging)\n",
        "abs_error_vorcnn = np.abs(error_vorcnn)\n",
        "abs_error_convlstm = np.abs(error_convlstm)\n",
        "abs_error_ced = np.abs(error_ced)\n",
        "\n",
        "mask = (y_test[0]==0)\n",
        "\n",
        "# Plotting functions\n",
        "def plot_image_and_colorbar(ax, image, title, cmap='viridis', vmin=None, vmax=None):\n",
        "    if mask is not None:\n",
        "        image = np.ma.array(image, mask=mask)\n",
        "    im = ax.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.08)\n",
        "    cbar = plt.colorbar(im, cax=cax)\n",
        "    cbar.ax.tick_params(labelsize=8)\n",
        "    return cbar\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(4, 3, figsize=(28, 15))\n",
        "\n",
        "\n",
        "vmin_error = np.min([np.min(abs_error_tD_kriging), np.min(abs_error_tD_kriging)])\n",
        "vmax_error = np.max([np.max(abs_error_tD_kriging), np.max(abs_error_tD_kriging)])+1  # Subtract 3 for better visualization\n",
        "\n",
        "\n",
        "\n",
        "# True Field, 3D Kriging, 2D Kriging\n",
        "plot_image_and_colorbar(axs[0, 0], true_field, 'True Field', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "\n",
        "\n",
        "plot_image_and_colorbar(axs[0, 1], TD_kriging_rotated, '3D Kriging', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[0, 2], tD_kriging_rotated, '2D Kriging', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "\n",
        "# Empty spaces\n",
        "axs[1, 0].axis('off')\n",
        "\n",
        "\n",
        "plot_image_and_colorbar(axs[1, 1], abs_error_TD_kriging, 'Error Map (3D Kriging)', vmin=vmin_error, vmax=vmax_error)\n",
        "\n",
        "\n",
        "plot_image_and_colorbar(axs[1, 2], abs_error_tD_kriging, 'Error Map (2D Kriging)', vmin=vmin_error, vmax=vmax_error)\n",
        "\n",
        "# Vor-CNN, ConvLSTM, CED-LSTM\n",
        "plot_image_and_colorbar(axs[2, 0], vorcnn, 'Vor-CNN', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[2, 1], convlstm, 'ConvLSTM', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[2, 2], ced, 'CED-LSTM', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "\n",
        "# Empty spaces\n",
        "cbar3 = plot_image_and_colorbar(axs[3, 0], abs_error_vorcnn, 'Error Map (Vor-CNN)', vmin=vmin_error, vmax=vmax_error)\n",
        "\n",
        "# Error maps for Vor-CNN, ConvLSTM, CED-LSTM\n",
        "plot_image_and_colorbar(axs[3, 1], abs_error_convlstm, 'Error Map (ConvLSTM)', vmin=vmin_error, vmax=vmax_error)\n",
        "cbar4 = plot_image_and_colorbar(axs[3, 2], abs_error_ced, 'Error Map (CED-LSTM)', vmin=vmin_error, vmax=vmax_error)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TpJFc31HwfL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "mpl.rcParams['font.size'] = 12  # Increase font size\n",
        "mpl.rcParams['axes.labelsize'] = 10  # Increase label font size\n",
        "\n",
        "# Calculate errors\n",
        "error_TD_kriging = true_field - TD_kriging_rotated.reshape((180,360,1))\n",
        "error_tD_kriging = true_field - tD_kriging_rotated.reshape((180,360,1))\n",
        "error_vorcnn = true_field - vorcnn\n",
        "error_convlstm = true_field - convlstm\n",
        "error_ced = true_field - ced\n",
        "\n",
        "abs_error_TD_kriging = np.abs(error_TD_kriging)\n",
        "abs_error_tD_kriging = np.abs(error_tD_kriging)\n",
        "abs_error_vorcnn = np.abs(error_vorcnn)\n",
        "abs_error_convlstm = np.abs(error_convlstm)\n",
        "abs_error_ced = np.abs(error_ced)\n",
        "\n",
        "mask = (y_test[0]==0)\n",
        "\n",
        "# Plotting functions\n",
        "def plot_image_and_colorbar(ax, image, title, cmap='viridis', vmin=None, vmax=None):\n",
        "    if mask is not None:\n",
        "        image = np.ma.array(image, mask=mask)\n",
        "    im = ax.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(title, fontsize=16)  # Increase title font size\n",
        "    ax.axis('off')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.08)\n",
        "    cbar = plt.colorbar(im, cax=cax)\n",
        "    cbar.ax.tick_params(labelsize=14)\n",
        "    return cbar\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(4, 3, figsize=(20, 13))\n",
        "\n",
        "\n",
        "vmin_error = np.min([np.min(abs_error_tD_kriging), np.min(abs_error_tD_kriging)])\n",
        "vmax_error = np.max([np.max(abs_error_tD_kriging), np.max(abs_error_tD_kriging)])+1  # Subtract 3 for better visualization\n",
        "\n",
        "\n",
        "\n",
        "# True Field, 3D Kriging, 2D Kriging\n",
        "plot_image_and_colorbar(axs[0, 0], true_field, 'True Field', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "\n",
        "\n",
        "plot_image_and_colorbar(axs[0, 1], TD_kriging_rotated, '3D Kriging', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[0, 2], tD_kriging_rotated, '2D Kriging', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "\n",
        "# Empty spaces\n",
        "axs[1, 0].axis('off')\n",
        "\n",
        "\n",
        "plot_image_and_colorbar(axs[1, 1], abs_error_TD_kriging, 'Error Map (3D Kriging)', vmin=vmin_error, vmax=vmax_error)\n",
        "\n",
        "\n",
        "plot_image_and_colorbar(axs[1, 2], abs_error_tD_kriging, 'Error Map (2D Kriging)', vmin=vmin_error, vmax=vmax_error)\n",
        "\n",
        "# Vor-CNN, ConvLSTM, CED-LSTM\n",
        "plot_image_and_colorbar(axs[2, 0], vorcnn, 'Vor-CNN', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[2, 1], convlstm, 'ConvLSTM', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "plot_image_and_colorbar(axs[2, 2], ced, 'CED-LSTM', vmin=cbar1.vmin, vmax=cbar1.vmax)\n",
        "\n",
        "# Empty spaces\n",
        "cbar3 = plot_image_and_colorbar(axs[3, 0], abs_error_vorcnn, 'Error Map (Vor-CNN)', vmin=vmin_error, vmax=vmax_error)\n",
        "\n",
        "# Error maps for Vor-CNN, ConvLSTM, CED-LSTM\n",
        "plot_image_and_colorbar(axs[3, 1], abs_error_convlstm, 'Error Map (ConvLSTM)', vmin=vmin_error, vmax=vmax_error)\n",
        "cbar4 = plot_image_and_colorbar(axs[3, 2], abs_error_ced, 'Error Map (CED-LSTM)', vmin=vmin_error, vmax=vmax_error)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QYKGgCo40OPv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V100",
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}